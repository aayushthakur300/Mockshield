# import os
# import google.generativeai as genai
# import json
# import itertools
# import random
# import time
# import re
# from dotenv import load_dotenv

# # 1. Load Environment Variables
# load_dotenv()

# # 2. Check API Key & Configure Mock Mode
# api_key = os.getenv("GOOGLE_API_KEY")
# is_mock_mode = False

# if not api_key or "AIzaSy" in api_key and len(api_key) < 10: 
#     print("‚ö†Ô∏è SYSTEM STATUS: No valid Google API Key found. Running in SMART SIMULATION MODE.")
#     is_mock_mode = True
# else:
#     genai.configure(api_key=api_key)

# # 3. COMPREHENSIVE MODEL LIST (Round Robin Failover)
# ALL_MODELS = [
#     'models/gemini-2.0-flash',
#     'models/gemini-2.0-flash-001',
#     'models/gemini-flash-latest',
#     'models/gemini-flash-lite-latest',
#     'models/gemini-2.5-flash',
#     'models/gemini-2.5-flash-lite',
#     'models/gemini-robotics-er-1.5-preview',

#     # --- TIER 2: 1.5 FLASH SERIES ---
#     'models/gemini-1.5-flash',
#     'models/gemini-1.5-flash-latest',
#     'models/gemini-1.5-flash-001',
#     'models/gemini-1.5-flash-002',
#     'models/gemini-1.5-flash-8b',
#     'models/gemini-1.5-flash-8b-latest',
#     'models/gemini-1.5-flash-8b-001',
    
#     # --- TIER 3: 1.5 PRO SERIES ---
#     'models/gemini-1.5-pro',
#     'models/gemini-1.5-pro-latest',
#     'models/gemini-1.5-pro-001',
#     'models/gemini-1.5-pro-002',
    
#     # --- TIER 5: LEGACY 1.0 PRO SERIES ---
#     'models/gemini-1.0-pro',
#     'models/gemini-1.0-pro-latest',
#     'models/gemini-1.0-pro-001',
#     'models/gemini-pro',
#     'models/gemini-pro-vision', 
    
#     # --- TIER 2: NEXT GEN (2.5) ---
#     'models/gemini-2.5-flash-preview-09-2025',
#     'models/gemini-2.5-flash-lite-preview-09-2025',
#     'models/gemini-2.5-flash-tts',

#     # --- TIER 3: HIGH INTELLIGENCE PRO MODELS ---
#     'models/gemini-2.5-pro',
#     'models/gemini-pro-latest',
#     'models/gemini-3-pro-preview',
#     'models/deep-research-pro-preview-12-2025',

#     # --- TIER 4: LIGHTWEIGHT / PREVIEW ---
#     'models/gemini-2.0-flash-lite',
#     'models/gemini-2.0-flash-lite-001',
#     'models/gemini-2.0-flash-lite-preview',
#     'models/gemini-2.0-flash-lite-preview-02-05',

#     # --- TIER 5: EXPERIMENTAL ---
#     'models/gemini-2.0-flash-exp',
#     'models/gemini-exp-1206',

#     # --- TIER 6: GEMMA (OPEN MODELS FALLBACK) ---
#     'models/gemma-3-27b-it',
#     'models/gemma-3-12b-it',
#     'models/gemma-3-4b-it',
#     'models/gemma-3-1b-it',
#     'models/gemma-3n-e4b-it',
#     'models/gemma-3n-e2b-it',

#     # --- TIER 6: GEMMA & OPEN MODELS ---
#     'models/gemma-2-27b-it',
#     'models/gemma-2-9b-it',
#     'models/gemma-2-2b-it',
    
#     # --- TIER 7: OBSCURE PREVIEWS (LAST RESORT) ---
#     'models/gemini-2.5-flash-native-audio-dialog',
#     'models/nano-banana-pro-preview',
      
#     # --- TIER 7: OBSCURE / LAST RESORT ---
#     'models/aqa'
# ]

# # Create an infinite cycle iterator to loop through models forever
# model_cycle = itertools.cycle(ALL_MODELS)

# def get_next_model():
#     """Returns the next model in the list (Round Robin)."""
#     model_name = next(model_cycle)
#     return model_name

# def clean_json_string(text):
#     """Sanitizes AI response to extract JSON."""
#     try:
#         text = text.replace("```json", "").replace("```", "")
#         # Regex to find the JSON object { ... } or array [ ... ]
#         if text.strip().startswith("{"):
#             match = re.search(r'\{.*\}', text, re.DOTALL)
#             if match: return match.group(0)
#         elif text.strip().startswith("["):
#             match = re.search(r'\[.*\]', text, re.DOTALL)
#             if match: return match.group(0)
#         return text
#     except:
#         return text

# def generate_with_failover(prompt, max_retries=50):
#     """
#     Tries to generate content. If a model fails, it IMMEDIATELY switches 
#     to the next one in the list and tries again.
#     """
#     attempts = 0
#     while attempts < max_retries:
#         current_model = get_next_model()
#         print(f"üîÑ Attempt {attempts+1}/{max_retries}: Trying model '{current_model}'...")
        
#         try:
#             model = genai.GenerativeModel(current_model)
#             response = model.generate_content(prompt)
#             print(f"‚úÖ SUCCESS using {current_model}")
#             return response.text
            
#         except Exception as e:
#             print(f"‚ùå FAILED with {current_model}: {e}")
#             attempts += 1
#             time.sleep(0.5)
            
#     raise Exception("All attempted models failed. API connectivity issue.")

# def evaluate_full_interview(transcript_data):
#     """
#     THE SENIOR EXPERT EVALUATOR.
#     Generates detailed Side-by-Side analysis.
#     """
#     # 1. Immediate Mock Mode Return
#     if is_mock_mode:
#         return {
#             "score": 75,
#             "summary": "Mock Mode: API Key missing. Good effort on basics.",
#             "silent_killers": ["Mock: Low energy"],
#             "roadmap": "Add API Key to see full analysis.",
#             "question_reviews": []
#         }

#     try:
#         # 2. Try Real AI Evaluation
#         transcript_text = json.dumps(transcript_data)

#         prompt = f"""
#         ACT AS: A Senior Technical Hiring Manager & Principal Software Engineer.
#         TASK: Conduct a forensic analysis of this interview.

#         TRANSCRIPT:
#         {transcript_text}

#         REQUIREMENTS:
#         1. **Global Score (0-100):** Be strict. 
#         2. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
#         3. **Per-Question Breakdown:** For EVERY question, provide:
#            - "question": The original question.
#            - "user_answer": The exact answer given.
#            - "score": 0-10 rating.
#            - "feedback": Critique of the user's answer.
#            - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth).

#         OUTPUT FORMAT (JSON ONLY):
#         {{
#             "score": <number>,
#             "summary": "<executive_summary_string>",
#             "silent_killers": ["<killer1>", "<killer2>"],
#             "roadmap": "<step_by_step_improvement_plan>",
#             "question_reviews": [
#                 {{
#                     "question": "...",
#                     "user_answer": "...",
#                     "score": 8,
#                     "feedback": "...",
#                     "ideal_answer": "..."
#                 }}
#             ]
#         }}
#         """

#         raw_text = generate_with_failover(prompt)
#         cleaned_text = clean_json_string(raw_text)
#         return json.loads(cleaned_text)

#     except Exception as e:
#         print(f"‚ùå EVALUATION CRASHED: {e}")
#         print("‚ö†Ô∏è SWITCHING TO SMART OFFLINE FALLBACK ENGINE")
        
#         # 3. CRASH-PROOF FALLBACK (Algorithmic Grading)
#         # This calculates a score based on answer length and heuristics so the UI never breaks.
#         total_words = 0
#         reviews = []
        
#         for item in transcript_data:
#             ans = str(item.get('answer', ''))
#             word_count = len(ans.split())
#             total_words += word_count
            
#             # Algorithmic Scoring Rule:
#             # - Short answer (<10 words) = 3/10
#             # - Medium answer (10-30 words) = 6/10
#             # - Long answer (>30 words) = 8/10
#             algo_score = 3
#             if word_count > 30: algo_score = 8
#             elif word_count > 10: algo_score = 6
            
#             reviews.append({
#                 "question": item.get('question', 'Unknown Question'),
#                 "user_answer": ans,
#                 "score": algo_score,
#                 "feedback": "Offline Analysis: Answer recorded successfully. Detailed AI critique unavailable due to network/API restrictions.",
#                 "ideal_answer": "A strong answer would include specific technical definitions, a real-world example, and a mention of trade-offs."
#             })

#         # Calculate global score (capped at 85 for offline mode)
#         final_score = min(85, max(40, int((total_words / len(transcript_data) if len(transcript_data) > 0 else 1) * 2)))

#         return {
#             "score": final_score,
#             "summary": "Offline Assessment Complete. The AI Engine is currently unreachable, so scoring is based on response metrics and keyword density.",
#             "silent_killers": ["Network/API Connectivity Issue", "Analysis limited to heuristics"],
#             "roadmap": "Check backend logs for '500 Internal Server Error'. Ensure Google API Key is valid and has quota.",
#             "question_reviews": reviews
#         }
#----------------------------------------------------------------------------------------------------------------
#----------------------------------------------------------------------------------------------------------------
#  ------------> 5 defense layer
# import os
# import google.generativeai as genai
# import json
# import itertools
# import random
# import time
# import re
# from dotenv import load_dotenv

# # 1. Load Environment Variables
# load_dotenv()

# # 2. Check API Key & Configure Mock Mode
# api_key = os.getenv("GOOGLE_API_KEY")
# is_mock_mode = False

# if not api_key or ("AIzaSy" in api_key and len(api_key) < 10):
#     print("‚ö†Ô∏è SYSTEM STATUS: No valid Google API Key found. Running in SMART SIMULATION MODE.")
#     is_mock_mode = True
# else:
#     genai.configure(api_key=api_key)

# # 3. COMPREHENSIVE MODEL LIST (Round Robin Failover)
# ALL_MODELS = [
#     'models/gemini-2.0-flash',
#     'models/gemini-2.0-flash-001',
#     'models/gemini-flash-latest',
#     'models/gemini-flash-lite-latest',
#     'models/gemini-2.5-flash',
#     'models/gemini-2.5-flash-lite',
#     'models/gemini-robotics-er-1.5-preview',
#     'models/gemini-1.5-flash',
#     'models/gemini-1.5-flash-latest',
#     'models/gemini-1.5-flash-001',
#     'models/gemini-1.5-flash-002',
#     'models/gemini-1.5-flash-8b',
#     'models/gemini-1.5-flash-8b-latest',
#     'models/gemini-1.5-flash-8b-001',
#     'models/gemini-1.5-pro',
#     'models/gemini-1.5-pro-latest',
#     'models/gemini-1.5-pro-001',
#     'models/gemini-1.5-pro-002',
#     'models/gemini-1.0-pro',
#     'models/gemini-1.0-pro-latest',
#     'models/gemini-1.0-pro-001',
#     'models/gemini-pro',
#     'models/gemini-pro-vision',
#     'models/gemini-2.5-flash-preview-09-2025',
#     'models/gemini-2.5-flash-lite-preview-09-2025',
#     'models/gemini-2.5-flash-tts',
#     'models/gemini-2.5-pro',
#     'models/gemini-pro-latest',
#     'models/gemini-3-pro-preview',
#     'models/deep-research-pro-preview-12-2025',
#     'models/gemini-2.0-flash-lite',
#     'models/gemini-2.0-flash-lite-001',
#     'models/gemini-2.0-flash-lite-preview',
#     'models/gemini-2.0-flash-lite-preview-02-05',
#     'models/gemini-2.0-flash-exp',
#     'models/gemini-exp-1206',
#     'models/gemma-3-27b-it',
#     'models/gemma-3-12b-it',
#     'models/gemma-3-4b-it',
#     'models/gemma-3-1b-it',
#     'models/gemma-3n-e4b-it',
#     'models/gemma-3n-e2b-it',
#     'models/gemma-2-27b-it',
#     'models/gemma-2-9b-it',
#     'models/gemma-2-2b-it',
#     'models/gemini-2.5-flash-native-audio-dialog',
#     'models/nano-banana-pro-preview',
#     'models/aqa'
# ]

# # Create an infinite cycle iterator to loop through models forever
# model_cycle = itertools.cycle(ALL_MODELS)

# def get_next_model():
#     """Returns the next model in the list (Round Robin)."""
#     model_name = next(model_cycle)
#     return model_name

# def clean_json_string(text):
#     """Sanitizes AI response to extract JSON."""
#     try:
#         # Remove markdown code blocks
#         text = text.replace("```json", "").replace("```", "")
        
#         # Regex to find the JSON object { ... } or array [ ... ]
#         if text.strip().startswith("{"):
#             match = re.search(r'\{.*\}', text, re.DOTALL)
#             if match: return match.group(0)
#         elif text.strip().startswith("["):
#             match = re.search(r'\[.*\]', text, re.DOTALL)
#             if match: return match.group(0)
#         return text
#     except:
#         return text

# def generate_with_failover(prompt, max_retries=55):
#     """
#     Tries to generate content. If a model fails, it IMMEDIATELY switches 
#     to the next one in the list and tries again.
#     """
#     attempts = 0
#     while attempts < max_retries:
#         current_model = get_next_model()
#         print(f"üîÑ Attempt {attempts+1}/{max_retries}: Trying model '{current_model}'...")
        
#         try:
#             model = genai.GenerativeModel(current_model)
#             response = model.generate_content(prompt)
#             print(f"‚úÖ SUCCESS using {current_model}")
#             return response.text
            
#         except Exception as e:
#             print(f"‚ùå FAILED with {current_model}: {e}")
#             attempts += 1
#             time.sleep(0.5)
            
#     raise Exception("All attempted models failed. API connectivity issue.")

# def generate_interview_questions(topic, difficulty="Senior", count=5):
#     """
#     SIMULATED DATABASE QUERY ENGINE.
#     Generates structured interview questions using the hybrid Senior Project Manager prompt.
#     """
#     if is_mock_mode:
#         return [{"id": 1, "type": "coding", "question": f"Mock Question regarding {topic}"}]

#     # Calculate mandatory distribution
#     coding_target = max(1, int(count * 0.4))  # Ensure at least 40% coding
#     theory_target = count - coding_target
#     unique_seed = random.randint(10000, 99999)

#     # EXTREME HYBRID MODEL PROMPT
#     prompt = f"""
#     ### SYSTEM ROLE: SENIOR PROJECT MANAGER & GLOBAL HEAD OF ENGINEERING TALENT
#     *** MODE: EXTREME HYBRID MODEL - DATABASE QUERY SIMULATION - {topic.upper()} (ID: {unique_seed}) ***
        
#     YOUR ROLE: You are executing a SQL query against a database of 50,000+ {topic} questions.
#     QUERY: SELECT * FROM {topic}_questions WHERE difficulty = '{difficulty}' ORDER BY RANDOM() LIMIT {count};
        
#     TASK: Return the result of this query as a strict JSON array.
        
#     MANDATORY DISTRIBUTION:
#     1. **CODING CHALLENGES (Exactly {coding_target} Questions):**
#        - MUST be pure coding tasks suitable for {difficulty} level.
#        - Focus on algorithms, data structures, or system design scenarios.
       
#     2. **THEORY & CONCEPTS (Exactly {theory_target} Questions):**
#        - Deep dive into internals of {topic}.
#        - Ask about trade-offs, underlying architecture, or best practices.
       
#     PRE-FLIGHT CHECK:
#     - Are there exactly {coding_target} coding questions?
#     - Is the difficulty strictly {difficulty}?
#     - Are the questions designed to filter out average candidates?
    
#     OUTPUT FORMAT (JSON ONLY):
#     [
#         {{
#             "id": 1,
#             "type": "coding",
#             "question": "..." 
#         }},
#         {{
#             "id": 2,
#             "type": "theory",
#             "question": "..." 
#         }}
#     ]
#     """

#     try:
#         raw_text = generate_with_failover(prompt)
#         cleaned_text = clean_json_string(raw_text)
#         return json.loads(cleaned_text)
#     except Exception as e:
#         print(f"‚ùå QUESTION GENERATION ERROR: {e}")
#         return []

# def evaluate_full_interview(transcript_data):
#     """
#     THE SENIOR EXPERT EVALUATOR.
#     Generates detailed Side-by-Side analysis using the Global Head of Talent persona.
#     """
#     # 1. Immediate Mock Mode Return
#     if is_mock_mode:
#         return {
#             "score": 75,
#             "summary": "Mock Mode: API Key missing. Good effort on basics.",
#             "silent_killers": ["Mock: Low energy"],
#             "roadmap": "Add API Key to see full analysis.",
#             "question_reviews": []
#         }

#     try:
#         # 2. Try Real AI Evaluation
#         transcript_text = json.dumps(transcript_data)

#         # FORENSIC GAP ANALYSIS PROMPT
#         prompt = f"""
#         ### ROLE & SYSTEM CONTEXT:
#         ACT AS: The 'Global Head of Engineering Talent' & 'Principal Technical Architect'.
#         CAPABILITIES: You possess 100% mastery of Technical Stacks (Code/Architecture), HR Behavioral Psychology, and Aptitude Evaluation.
#         TASK: Conduct a forensic "Gap Analysis" of the following candidate interview transcript.

#         ### TRANSCRIPT DATA:
#         {transcript_text}

#         ANALYSIS PROTOCOL (THE "SIDE-BY-SIDE" EVALUATION)
#         For every question/answer pair, you MUST perform a "Side-by-Side" evaluation using these three lenses:

#         **LENS A: The Technical Autopsy (The Reality Check)**
#         - **Does it work?** (Baseline functionality).
#         - **Is it scalable?** (O(n) vs O(n^2), database load, memory leaks).
#         - **Security & Edge Cases:** Did they mention input validation, race conditions, or failure states? (Seniority Indicator).
#         - **THE GAP:** What would a Staff Engineer (L6+) have said that this candidate missed?

#         **LENS B: The Psychological Profile (The "Silent Killers")**
#         - **Detect "Resume Padding":** Does the depth of the answer match the confidence of the claim?
#         - **Detect "Hedging":** Phrases like "I guess," "maybe," or vague generalizations without concrete examples.
#         - **Detect "Buzzword Stuffing":** Using terms like "Microservices" or "AI" without explaining *why*.

#         **LENS C: The Communication Delta**
#         - **Structure:** Was the answer structured (STAR method) or rambling?
#         - **Ownership:** Did they drive the conversation or passively wait for prompts?

#         ### SCORING CRITERIA:
#         - **9-10:** Flawless. Mentions trade-offs, edge cases, and business impact.
#         - **6-8:** Correct but shallow. Textbook answer without real-world depth.
#         - **0-5:** Incorrect, vague, or dangerous engineering practices.

#         REQUIREMENTS:
#         1. **Global Score (0-100):** Be strict. 
#         2. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
#         3. **Per-Question Breakdown:** For EVERY question, provide:
#             - "question": The original question.
#             - "user_answer": The exact answer given.
#             - "score": 0-10 rating.
#             - "feedback": Critique of the user's answer.
#             - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth).

#         CRITICAL JSON FORMATTING RULES:
#         - Output ONLY valid JSON.
#         - **ESCAPE ALL BACKSLASHES:** If you write code with '\\n', you MUST write it as '\\\\n'. 
#         - Do not include any text before or after the JSON.

#         OUTPUT FORMAT (JSON ONLY):
#         {{
#             "score": <number>,
#             "summary": "<executive_summary_string>",
#             "silent_killers": ["<killer1>", "<killer2>"],
#             "roadmap": "<step_by_step_improvement_plan>",
#             "question_reviews": [
#                 {{
#                     "question": "...",
#                     "user_answer": "...",
#                     "score": 8,
#                     "feedback": "...",
#                     "ideal_answer": "..."
#                 }}
#             ]
#         }}
#         """

#         raw_text = generate_with_failover(prompt)
#         cleaned_text = clean_json_string(raw_text)
        
#         # --- SELF-HEALING JSON PARSER (THE PERMANENT FIX) ---
#         try:
#             return json.loads(cleaned_text)
#         except json.JSONDecodeError:
#             print("‚ö†Ô∏è JSON Parse Error: Invalid Escape detected. Attempting Regex Repair...")
#             # Repair Strategy 1: Fix single backslashes that aren't valid escapes
#             repaired_text = re.sub(r'\\(?![/\\bfnrtu"])', r'\\\\', cleaned_text)
            
#             try:
#                 return json.loads(repaired_text)
#             except json.JSONDecodeError:
#                 print("‚ö†Ô∏è Regex Repair failed. Attempting Nuclear Repair (Double-Escape All)...")
#                 # Repair Strategy 2: Nuclear Option. Just double escape ALL backslashes.
#                 nuclear_text = cleaned_text.replace('\\', '\\\\')
#                 try:
#                     return json.loads(nuclear_text)
#                 except:
#                      raise Exception("All JSON Repair strategies failed.")

#     except Exception as e:
#         print(f"‚ùå EVALUATION CRASHED: {e}")
#         print("‚ö†Ô∏è SWITCHING TO SMART OFFLINE FALLBACK ENGINE")
        
#         # 3. CRASH-PROOF FALLBACK (Algorithmic Grading)
#         # This ensures the UI NEVER breaks, even if the AI outputs garbage.
#         total_words = 0
#         reviews = []
        
        
#         for item in transcript_data:
#             ans = str(item.get('answer', ''))
#             word_count = len(ans.split())
#             total_words += word_count
            
#             # Simple Heuristic Scoring
#             algo_score = 4
#             if word_count > 30: algo_score = 8
#             elif word_count > 10: algo_score = 6
            
#             reviews.append({
#                 "question": item.get('question', 'Unknown Question'),
#                 "user_answer": ans,
#                 "score": algo_score,
#                 "feedback": "Offline Analysis: Answer recorded. Deep AI analysis unavailable due to network/parsing constraints.",
#                 "ideal_answer": "A strong answer should define the core concept, provide a use case, and mention trade-offs."
#             })

#         # Calculate rough global score
#         final_score = min(88, max(40, int((total_words / (len(transcript_data) or 1)) * 2)))

#         return {
#             "score": final_score,
#             "summary": "Offline Assessment: The AI Engine is temporarily unreachable or returned invalid data. Scoring is based on response metrics.",
#             "silent_killers": ["Network/API Connectivity Issue", "Heuristic Analysis Mode"],
#             "roadmap": "Check backend logs for JSON formatting errors. Ensure Google API Key is active.",
#             "question_reviews": reviews
#         }

# # --- EXAMPLE USAGE BLOCK ---
# if __name__ == "__main__":
#     print("--- 1. Generating Questions (DB Simulation) ---")
#     questions = generate_interview_questions("Python", "Senior", 4)
#     print(json.dumps(questions, indent=2))
    
#     # Simulate a user answering
#     transcript = []
#     for q in questions:
#         transcript.append({
#             "question": q['question'],
#             "answer": "I would use a hash map for O(1) lookups." # Mock answer
#         })
        
#     print("\n--- 2. Evaluating Interview (Hiring Manager Persona) ---")
#     analysis = evaluate_full_interview(transcript)
#     print(json.dumps(analysis, indent=2))
#-----------------------------------------------------------------------------------------------------------------------
# -----------------------------> 7 defense layer
# filename: evaluator.py

# import os
# import google.generativeai as genai
# import json
# import itertools
# import random
# import time
# import re
# import traceback
# import ast
# import concurrent.futures
# import difflib
# import logging
# import threading
# import sys
# from collections import deque
# from datetime import datetime
# from dotenv import load_dotenv

# # ==========================================
# # 1. SYSTEM CONFIGURATION & TELEMETRY
# # ==========================================
# load_dotenv()

# # --- WINDOWS UNICODE FIX ---
# # Forces the system to use UTF-8 for output to handle emojis (üîÑ, ‚ùå, ‚úÖ)
# # This prevents UnicodeEncodeError on Windows consoles.
# if sys.platform.startswith('win'):
#     try:
#         sys.stdout.reconfigure(encoding='utf-8')
#         sys.stderr.reconfigure(encoding='utf-8')
#     except AttributeError:
#         # Fallback for older python versions if necessary, though 3.13 supports reconfigure
#         pass

# # Setup Structured Logging (Telemetry Fix)
# # Writes to both console and a rotating log file for audit trails.
# # explicitly setting encoding='utf-8' for the file handler.
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - [TELEMETRY] %(message)s',
#     handlers=[
#         logging.FileHandler("system_audit.log", encoding='utf-8'),
#         logging.StreamHandler(sys.stdout)
#     ]
# )

# # Check API Key & Configure Mock Mode
# api_key = os.getenv("GOOGLE_API_KEY")
# is_mock_mode = False

# if not api_key or ("AIzaSy" in api_key and len(api_key) < 10):
#     logging.warning("‚ö†Ô∏è SYSTEM STATUS: No valid Google API Key found. Running in SMART SIMULATION MODE.")
#     is_mock_mode = True
# else:
#     genai.configure(api_key=api_key)

# # ==========================================
# # 2. COMPREHENSIVE MODEL LIST (Round Robin)
# # ==========================================
# ALL_MODELS = [
#     'models/gemini-2.0-flash',
#     'models/gemini-2.0-flash-001',
#     'models/gemini-flash-latest',
#     'models/gemini-flash-lite-latest',
#     'models/gemini-2.5-flash',
#     'models/gemini-2.5-flash-lite',
#     'models/gemini-robotics-er-1.5-preview',
#     'models/gemini-1.5-flash',
#     'models/gemini-1.5-flash-latest',
#     'models/gemini-1.5-flash-001',
#     'models/gemini-1.5-flash-002',
#     'models/gemini-1.5-flash-8b',
#     'models/gemini-1.5-flash-8b-latest',
#     'models/gemini-1.5-flash-8b-001',
#     'models/gemini-1.5-pro',
#     'models/gemini-1.5-pro-latest',
#     'models/gemini-1.5-pro-001',
#     'models/gemini-1.5-pro-002',
#     'models/gemini-1.0-pro',
#     'models/gemini-1.0-pro-latest',
#     'models/gemini-1.0-pro-001',
#     'models/gemini-pro',
#     'models/gemini-pro-vision',
#     'models/gemini-2.5-flash-preview-09-2025',
#     'models/gemini-2.5-flash-lite-preview-09-2025',
#     'models/gemini-2.5-flash-tts',
#     'models/gemini-2.5-pro',
#     'models/gemini-pro-latest',
#     'models/gemini-3-pro-preview',
#     'models/deep-research-pro-preview-12-2025',
#     'models/gemini-2.0-flash-lite',
#     'models/gemini-2.0-flash-lite-001',
#     'models/gemini-2.0-flash-lite-preview',
#     'models/gemini-2.0-flash-lite-preview-02-05',
#     'models/gemini-2.0-flash-exp',
#     'models/gemini-exp-1206',
#     'models/gemma-3-27b-it',
#     'models/gemma-3-12b-it',
#     'models/gemma-3-4b-it',
#     'models/gemma-3-1b-it',
#     'models/gemma-3n-e4b-it',
#     'models/gemma-3n-e2b-it',
#     'models/gemma-2-27b-it',
#     'models/gemma-2-9b-it',
#     'models/gemma-2-2b-it',
#     'models/gemini-2.5-flash-native-audio-dialog',
#     'models/nano-banana-pro-preview',
#     'models/aqa'
# ]

# model_cycle = itertools.cycle(ALL_MODELS)

# def get_next_model():
#     """Returns the next model in the list (Round Robin)."""
#     return next(model_cycle)

# # ==========================================
# # 3. HELPER FUNCTIONS & SECURITY
# # ==========================================

# def sanitize_prompt_input(text):
#     """
#     SECURITY FIX: Sanitizes inputs to prevent Prompt Injection.
#     """
#     if not isinstance(text, str):
#         return str(text)
#     sanitized = re.sub(r'[\'"\n\r]', '', text)
#     return sanitized.strip()[:100]

# def normalize_text_for_comparison(text):
#     """
#     NEW: Aggressive normalization for duplicate detection.
#     Removes punctuation, converts to lowercase, strips whitespace.
#     """
#     if not text: return ""
#     # Remove everything except alphanumeric chars
#     return re.sub(r'[\W_]+', '', text.lower())

# def sanitize_transcript_for_prompt(transcript_data):
#     """
#     SECURITY FIX: Prevents Transcript Injection.
#     User answers are sanitized to prevent them from breaking the JSON structure 
#     or injecting system commands.
#     """
#     clean_data = []
#     if isinstance(transcript_data, list):
#         for item in transcript_data:
#             clean_item = {}
#             for k, v in item.items():
#                 # Limit length and remove potential control sequences
#                 s_val = str(v)[:2000] 
#                 s_val = s_val.replace("```", "").replace("###", "") # Remove system delimiters
#                 clean_item[k] = s_val
#             clean_data.append(clean_item)
#     return json.dumps(clean_data)

# # ==========================================
# # 4. ROBUST HISTORY MANAGER (Memory Leak Fix)
# # ==========================================

# class HistoryManager:
#     """
#     MANAGES LONG-TERM MEMORY with CAPPED STORAGE.
#     File: question_history.json
#     """
#     FILE_PATH = "question_history.json"
#     MAX_HISTORY_PER_TOPIC = 500  # Cap history size to prevent slow IO
    
#     def __init__(self):
#         self._ensure_file_exists()

#     def _ensure_file_exists(self):
#         if not os.path.exists(self.FILE_PATH):
#             try:
#                 with open(self.FILE_PATH, 'w', encoding='utf-8') as f:
#                     json.dump({"meta": "Interview History", "topics": {}}, f, indent=2)
#             except Exception as e:
#                 logging.error(f"HISTORY INIT ERROR: {e}")

#     def load_history(self):
#         try:
#             with open(self.FILE_PATH, 'r', encoding='utf-8') as f:
#                 return json.load(f)
#         except (json.JSONDecodeError, IOError):
#             return {"meta": "Interview History", "topics": {}}

#     def save_history(self, history_data):
#         try:
#             with open(self.FILE_PATH, 'w', encoding='utf-8') as f:
#                 json.dump(history_data, f, indent=2)
#         except Exception as e:
#             logging.error(f"HISTORY SAVE ERROR: {e}")

#     def get_past_questions(self, topic):
#         """Retrieves flat list of past questions for a specific topic."""
#         data = self.load_history()
#         topic_key = topic.lower().strip()
#         if topic_key in data.get("topics", {}):
#             return data["topics"][topic_key]
#         return []

#     def add_questions(self, topic, new_questions):
#         """Adds new questions with FIFO Rolling Window enforcement."""
#         data = self.load_history()
#         topic_key = topic.lower().strip()
        
#         if "topics" not in data:
#             data["topics"] = {}
#         if topic_key not in data["topics"]:
#             data["topics"][topic_key] = []
            
#         q_texts = [q['question'] for q in new_questions if 'question' in q]
        
#         # Extend list
#         data["topics"][topic_key].extend(q_texts)
        
#         # ENFORCE CAP: Keep only the last N items
#         if len(data["topics"][topic_key]) > self.MAX_HISTORY_PER_TOPIC:
#             data["topics"][topic_key] = data["topics"][topic_key][-self.MAX_HISTORY_PER_TOPIC:]
            
#         self.save_history(data)

#     def is_duplicate(self, new_question_text, past_questions, threshold=0.85):
#         """
#         UPGRADED MATCHING: Uses normalized strings to catch rephrasing.
#         """
#         if not new_question_text: return True
        
#         norm_new = normalize_text_for_comparison(new_question_text)
        
#         for past_q in past_questions:
#             # First pass: Direct normalized comparison (Fast)
#             norm_past = normalize_text_for_comparison(past_q)
#             if norm_new == norm_past:
#                 return True
                
#             # Second pass: Fuzzy Match (Slow but accurate)
#             # Only run fuzzy match if lengths are roughly similar to save CPU
#             if abs(len(norm_new) - len(norm_past)) < 20:
#                 similarity = difflib.SequenceMatcher(None, new_question_text, past_q).ratio()
#                 if similarity > threshold:
#                     return True
#         return False

# history_system = HistoryManager()

# # ==========================================
# # 5. 7-LAYER DEFENSE ENGINE (Data Integrity)
# # ==========================================

# def clean_json_string(text):
#     return sanitize_control_chars(text)

# def sanitize_control_chars(text):
#     text = re.sub(r'```[a-zA-Z]*\n', '', text)
#     text = text.replace("```", "")
#     return "".join(ch for ch in text if ord(ch) >= 32 or ch in "\n\r\t")

# def balance_brackets(text):
#     stack = []
#     for char in text:
#         if char in '{[':
#             stack.append(char)
#         elif char in '}]':
#             if stack:
#                 last = stack[-1]
#                 if (char == '}' and last == '{') or (char == ']' and last == '['):
#                     stack.pop()
#     balanced = text
#     while stack:
#         opener = stack.pop()
#         if opener == '{': balanced += '}'
#         elif opener == '[': balanced += ']'
#     return balanced

# def extract_json_candidates(text):
#     candidates = []
#     matches = list(re.finditer(r'(\{.*\}|\[.*\])', text, re.DOTALL))
#     for m in matches:
#         candidates.append(m.group(0))
#     if not candidates:
#         start_brace = text.find('{')
#         start_bracket = text.find('[')
#         if start_brace != -1: candidates.append(text[start_brace:])
#         elif start_bracket != -1: candidates.append(text[start_bracket:])
#     return candidates if candidates else [text]

# def enforce_schema(data, expected_type):
#     if expected_type == 'list':
#         if isinstance(data, list): return data
#         if isinstance(data, dict):
#             for key in data:
#                 if isinstance(data[key], list): return data[key]
#             return [data]
#     if expected_type == 'dict':
#         if isinstance(data, dict): return data
#         if isinstance(data, list) and len(data) > 0:
#             return data[0]
#     return data

# def safe_literal_eval(candidate):
#     """
#     SECURITY FIX: Safer alternative to bare ast.literal_eval.
#     Prevents exponential expansion attacks.
#     """
#     # 1. Length Limit Check
#     if len(candidate) > 50000: # 50KB limit for single JSON object
#         raise ValueError("Candidate too large for safe evaluation")
        
#     # 2. Syntactic Check (ensure it's not arbitrary python code like __import__)
#     # AST parse will fail if it's not a literal structure
#     return ast.literal_eval(candidate)

# def bulletproof_json_parser(raw_text, expected_type='dict'):
#     """
#     THE MASTER CONTROLLER (7 STRATEGIES).
#     """
#     clean_text = sanitize_control_chars(raw_text)
#     candidates = extract_json_candidates(clean_text)

#     for candidate in candidates:
#         try: return enforce_schema(json.loads(candidate), expected_type)
#         except: pass

#         try:
#             fixed = re.sub(r',\s*([\]}])', r'\1', candidate)
#             return enforce_schema(json.loads(fixed), expected_type)
#         except: pass

#         try:
#             # UPGRADED: Uses safe_literal_eval
#             return enforce_schema(safe_literal_eval(candidate), expected_type)
#         except: pass

#         try:
#             fixed = re.sub(r'\\(?![/\\bfnrtu"])', r'\\\\', candidate)
#             return enforce_schema(json.loads(fixed), expected_type)
#         except: pass

#         try:
#             balanced = balance_brackets(candidate)
#             return enforce_schema(json.loads(balanced), expected_type)
#         except: pass
        
#         if expected_type == 'list':
#             try:
#                 obj_matches = re.findall(r'\{.*?\}', candidate, re.DOTALL)
#                 reconstructed_list = []
#                 for obj_str in obj_matches:
#                     try:
#                         obj = json.loads(obj_str)
#                         reconstructed_list.append(obj)
#                     except:
#                         try:
#                              obj = safe_literal_eval(obj_str)
#                              reconstructed_list.append(obj)
#                         except: pass
#                 if reconstructed_list:
#                     return reconstructed_list
#             except: pass

#     # Last Resort Regex
#     if expected_type == 'dict':
#         try:
#             score_match = re.search(r'("score"|score)\s*[:=]\s*(\d+)', raw_text)
#             return {
#                 "score": int(score_match.group(2)) if score_match else 0,
#                 "summary": "CRITICAL PARSE FAILURE: Data extracted via Regex Fallback.",
#                 "is_fallback": True,
#                 "question_reviews": [],
#                 "silent_killers": [],
#                 "roadmap": "Parse failure."
#             }
#         except: pass

#     if expected_type == 'list': return []
#     return {}

# # ==========================================
# # 6. HARD TIMEOUT ENFORCER & ZOMBIE GUARD
# # ==========================================

# # Global Semaphore to prevent Self-DOS from zombie threads
# # Only allow 3 concurrent generation requests system-wide.
# CONCURRENCY_GUARD = threading.Semaphore(3)

# def generate_with_timeout_protection(model, prompt, timeout=25):
#     """
#     Wraps the API call. Uses Semaphore to block if too many threads are hanging.
#     """
#     # Try to acquire semaphore without blocking indefinitely (Fail fast if overloaded)
#     if not CONCURRENCY_GUARD.acquire(timeout=5):
#         raise SystemError("SYSTEM OVERLOAD: Too many active requests. Backing off.")
        
#     try:
#         with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(model.generate_content, prompt)
#             try:
#                 response = future.result(timeout=timeout)
#                 return response
#             except concurrent.futures.TimeoutError:
#                 # Thread will continue in background (zombie), but we move on.
#                 # Semaphore is released in finally block.
#                 logging.warning("Thread Timeout: Creating zombie thread.")
#                 raise TimeoutError(f"Model execution exceeded {timeout}s.")
#             except Exception as e:
#                 raise e
#     finally:
#         CONCURRENCY_GUARD.release()

# def generate_with_failover(prompt, max_retries=55):
#     """
#     Round-Robin Failover with Semaphore Protection.
#     """
#     attempts = 0
#     while attempts < max_retries:
#         current_model = get_next_model()
#         logging.info(f"üîÑ Attempt {attempts+1}/{max_retries}: Trying model '{current_model}'...")
        
#         try:
#             model = genai.GenerativeModel(current_model)
#             response = generate_with_timeout_protection(model, prompt, timeout=30)
            
#             if not response.text:
#                 raise ValueError("Empty response received")
                
#             logging.info(f"‚úÖ SUCCESS using {current_model}")
#             return response.text
            
#         except TimeoutError:
#              logging.warning(f"‚è≥ TIMEOUT with {current_model}. Switching...")
#              attempts += 1
#              time.sleep(0.5)
#         except SystemError as se:
#              # Critical Overload - Wait longer
#              logging.critical(f"üõë {se}")
#              time.sleep(2)
#              attempts += 1
#         except Exception as e:
#             logging.error(f"‚ùå FAILED with {current_model}: {str(e)[:100]}")
#             attempts += 1
#             time.sleep(0.5)
            
#     raise Exception("All attempted models failed or timed out.")

# # ==========================================
# # 7. MAIN LOGIC: GENERATION & EVALUATION
# # ==========================================

# def generate_interview_questions(topic, difficulty="Senior", count=5):
#     """
#     SIMULATED DATABASE QUERY ENGINE.
#     Includes History Manager, Prompt Injection protection, and Buffer/Filter Logic.
#     """
#     if is_mock_mode:
#         return [{"id": 1, "type": "coding", "question": f"Mock Question regarding {topic}"}]

#     # --- SECURITY SANITIZATION ---
#     clean_topic = sanitize_prompt_input(topic)
#     clean_difficulty = sanitize_prompt_input(difficulty)

#     # --- HISTORY RETRIEVAL & SMART INJECTION ---
#     # Retrieve past questions to forbid them in the prompt
#     past_questions = history_system.get_past_questions(clean_topic)
    
#     # Limit past questions token usage (send last 20)
#     recent_history = past_questions[-20:] if past_questions else []
    
#     exclusion_directive = ""
#     if recent_history:
#         exclusion_directive = f"### EXCLUSION PROTOCOL (SMART FILTER)\nDO NOT generate any questions similar to the following:\n{json.dumps(recent_history)}"

#     # --- DUPLICATION FILTER: BUFFER STRATEGY ---
#     # Request extra questions to allow local filtering of duplicates
#     buffer_count = 3
#     total_request_count = count + buffer_count

#     # Calculate mandatory distribution based on buffer size
#     coding_target = max(1, int(total_request_count * 0.4)) 
#     theory_target = total_request_count - coding_target
#     unique_seed = random.randint(10000, 99999)

#     prompt = f"""
#     ### SYSTEM OVERRIDE
#     - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here are the questions").
#     - MANDATORY: Output must be RAW JSON only.
#     - ESCAPING RULES: Double-escape all backslashes (\\\\ -> \\\\\\\\). Escape newlines in strings (\\n -> \\\\n).
    
#     {exclusion_directive}

#     ### SYSTEM ROLE: SENIOR PROJECT MANAGER & GLOBAL HEAD OF ENGINEERING TALENT
#     *** MODE: EXTREME HYBRID MODEL - DATABASE QUERY SIMULATION - {clean_topic.upper()} (ID: {unique_seed}) ***
        
#     YOUR ROLE: You are executing a SQL query against a database of 100,000+ {clean_topic} questions.
    
#     QUERY: SELECT * FROM {clean_topic}_questions WHERE difficulty = '{clean_difficulty}' AND is_cliche = FALSE ORDER BY RANDOM(SEED={unique_seed}) LIMIT {total_request_count};
      
#     TASK: Return the result of this query as a strict JSON array.
        
#     MANDATORY DISTRIBUTION:
#     1. **CODING CHALLENGES (Exactly {coding_target} Questions):**
#         - MUST be pure coding tasks suitable for {clean_difficulty} level.
#         - Focus on algorithms, data structures, or system design scenarios.
        
#     2. **THEORY & CONCEPTS (Exactly {theory_target} Questions):**
#         - Deep dive into internals of {clean_topic}.
#         - Ask about trade-offs, underlying architecture, or best practices.
        
#     PRE-FLIGHT CHECK:
#     - Are there exactly {coding_target} coding questions?
#     - Is the difficulty strictly {clean_difficulty}?
#     - Are the questions designed to filter out average candidates?
    
#     OUTPUT FORMAT (JSON ONLY):
#     [
#         {{
#             "id": 1,
#             "type": "coding",
#             "question": "..." 
#         }},
#         {{
#             "id": 2,
#             "type": "theory",
#             "question": "..." 
#         }}
#     ]
#     """

#     try:
#         raw_text = generate_with_failover(prompt)
#         # Uses the new 7-Layer Bulletproof Parser
#         raw_questions = bulletproof_json_parser(raw_text, expected_type='list')
        
#         # --- DUPLICATION FILTER LOGIC ---
#         final_questions = []
#         unique_added_count = 0
        
#         for q in raw_questions:
#             if unique_added_count >= count:
#                 break
                
#             q_text = q.get('question', '')
            
#             # Using Upgraded History System
#             if not history_system.is_duplicate(q_text, past_questions):
#                 # Assign new ID
#                 q['id'] = unique_added_count + 1
#                 final_questions.append(q)
#                 unique_added_count += 1
#             else:
#                 logging.info(f"üö´ DUPLICATE BLOCKED: {q_text[:40]}...")

#         # Fallback: If filtering removed too many, fill with remaining raw ones
#         if len(final_questions) < count:
#             logging.warning("‚ö†Ô∏è WARNING: History saturation. Reusing buffer to meet quota.")
#             remaining_needed = count - len(final_questions)
#             used_texts = [fq['question'] for fq in final_questions]
            
#             for q in raw_questions:
#                 if remaining_needed <= 0: break
#                 if q.get('question') not in used_texts:
#                     q['id'] = len(final_questions) + 1
#                     final_questions.append(q)
#                     remaining_needed -= 1
        
#         # --- SAVE TO HISTORY ---
#         history_system.add_questions(clean_topic, final_questions)

#         return final_questions

#     except Exception as e:
#         logging.error(f"‚ùå QUESTION GENERATION ERROR: {e}")
#         traceback.print_exc()
#         return [{"id": 0, "type": "error", "question": "Error generating questions. Please try again."}]

# def evaluate_full_interview(transcript_data):
#     """
#     THE SENIOR EXPERT EVALUATOR.
#     Includes Smart Offline Fallback if AI times out.
#     """
#     if is_mock_mode:
#         return {
#             "score": 75,
#             "summary": "Mock Mode: API Key missing. Good effort on basics.",
#             "silent_killers": ["Mock: Low energy"],
#             "roadmap": "Add API Key to see full analysis.",
#             "question_reviews": []
#         }

#     try:
#         # SECURITY: Sanitize user input before prompt injection
#         transcript_text = sanitize_transcript_for_prompt(transcript_data)

#         prompt = f"""
#         ### SYSTEM OVERRIDE
#         - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here is the analysis").
#         - MANDATORY: Output must be RAW JSON only.
#         - ESCAPING RULES: All backslashes must be double-escaped (\\\\ -> \\\\\\\\). All newlines within strings must be escaped (\\n -> \\\\n).

#         ### ROLE & SYSTEM CONTEXT:
#         ACT AS: The 'Global Head of Engineering Talent' & 'Principal Technical Architect'.
#         CAPABILITIES: You possess 100% mastery of Technical Stacks (Code/Architecture), HR Behavioral Psychology, and Aptitude Evaluation.
#         TASK: Conduct a forensic "Gap Analysis" of the following candidate interview transcript.

#         ### TRANSCRIPT DATA:
#         {transcript_text}

#         ANALYSIS PROTOCOL (THE "SIDE-BY-SIDE" EVALUATION)
#         For every question/answer pair, you MUST perform a "Side-by-Side" evaluation using these three lenses:

#         **LENS A: The Technical Autopsy (The Reality Check)**
#         - **Does it work?** (Baseline functionality).
#         - **Is it scalable?** (O(n) vs O(n^2), database load, memory leaks).
#         - **Security & Edge Cases:** Did they mention input validation, race conditions, or failure states? (Seniority Indicator).
#         - **THE GAP:** What would a Staff Engineer (L6+) have said that this candidate missed?

#         **LENS B: The Psychological Profile (The "Silent Killers")**
#         - **Detect "Resume Padding":** Does the depth of the answer match the confidence of the claim?
#         - **Detect "Hedging":** Phrases like "I guess," "maybe," or vague generalizations without concrete examples.
#         - **Detect "Buzzword Stuffing":** Using terms like "Microservices" or "AI" without explaining *why*.

#         **LENS C: The Communication Delta**
#         - **Structure:** Was the answer structured (STAR method) or rambling?
#         - **Ownership:** Did they drive the conversation or passively wait for prompts?

#         ### SCORING CRITERIA:
#         - **9-10:** Flawless. Mentions trade-offs, edge cases, and business impact.
#         - **6-8:** Correct but shallow. Textbook answer without real-world depth.
#         - **0-5:** Incorrect, vague, or dangerous engineering practices.

#         REQUIREMENTS:
#         1. **Global Score (0-100):** Be strict. 
#         2. **Executive Summary:** A concise verdict. 
#             - **STRICT CONSTRAINT:** Do NOT use generic phrases. 
#             - **MANDATORY:** Append 2-5 lines of specific, high-value improvement advice based on the interview performance.
#         3. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
#         4. **Per-Question Breakdown:** For EVERY question, provide:
#             - "question": The original question.
#             - "user_answer": The exact answer given.
#             - "score": 0-10 rating.
#             - "feedback": Critique of the user's answer.
#             - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth).

#         CRITICAL JSON FORMATTING RULES:
#         - Output ONLY valid JSON.
#         - **ESCAPE ALL BACKSLASHES:** If you write code with '\\n', you MUST write it as '\\\\n'. 
#         - Do not include any text before or after the JSON.

#         OUTPUT FORMAT (JSON ONLY):
#         {{
#             "score": <number>,
#             "summary": "<executive_summary_string>",
#             "silent_killers": ["<killer1>", "<killer2>"],
#             "roadmap": "<step_by_step_improvement_plan>",
#             "question_reviews": [
#                 {{
#                     "question": "...",
#                     "user_answer": "...",
#                     "score": 8,
#                     "feedback": "...",
#                     "ideal_answer": "..."
#                 }}
#             ]
#         }}
#         """

#         raw_text = generate_with_failover(prompt)
#         analysis = bulletproof_json_parser(raw_text, expected_type='dict')
#         return analysis

#     except Exception as e:
#         logging.error(f"‚ùå EVALUATION CRASHED or TIMED OUT: {e}")
#         logging.warning("‚ö†Ô∏è SWITCHING TO SMART OFFLINE FALLBACK ENGINE")
        
#         # 3. CRASH-PROOF FALLBACK (Algorithmic Grading)
#         # This executes if ALL AI models timeout or fail parsing
#         total_words = 0
#         reviews = []
        
#         # Robust iteration handling
#         safe_transcript = transcript_data if isinstance(transcript_data, list) else []
        
#         for item in safe_transcript:
#             ans = str(item.get('answer', ''))
#             word_count = len(ans.split())
#             total_words += word_count
            
#             # Simple Heuristic Scoring
#             algo_score = 4
#             if word_count > 30: algo_score = 8
#             elif word_count > 10: algo_score = 6
            
#             reviews.append({
#                 "question": item.get('question', 'Unknown Question'),
#                 "user_answer": ans,
#                 "score": algo_score,
#                 "feedback": "Offline Analysis: Answer recorded. High network latency prevented deep AI inspection.",
#                 "ideal_answer": "A strong answer should define the core concept, provide a use case, and mention trade-offs."
#             })

#         safe_len = len(safe_transcript) or 1
#         # Calculate algorithmic score capped at 88
#         final_score = min(88, max(40, int((total_words / safe_len) * 2)))

#         return {
#             "score": final_score,
#             "summary": "Offline Assessment: The AI Engine is temporarily unreachable due to network timeouts. Scoring is based on response metrics.",
#             "silent_killers": ["Network Timeout", "Offline Mode Active"],
#             "roadmap": "Check internet connection and API quotas. Retry later for full AI feedback.",
#             "question_reviews": reviews
#         }

# # ==========================================
# # 8. EXECUTION ENTRY POINT
# # ==========================================
# if __name__ == "__main__":
#     print("--- 1. Generating Questions (DB Simulation with History Awareness) ---")
    
#     # Example: Running the same topic twice to test Filter
#     topic_to_test = "DSA"
    
#     questions = generate_interview_questions(topic_to_test, "Senior", 4)
#     print(json.dumps(questions, indent=2))
    
#     # Simulate a user answering
#     transcript = []
#     if isinstance(questions, list):
#         for q in questions:
#             transcript.append({
#                 "question": q.get('question', 'Error'),
#                 "answer": "I would use a hash map for O(1) lookups but I need to consider collisions." # Mock answer
#             })
#     else:
#         transcript.append({"question": "Error", "answer": "Error"})
        
#     print("\n--- 2. Evaluating Interview (Hiring Manager Persona) ---")
#     analysis = evaluate_full_interview(transcript)
#     print(json.dumps(analysis, indent=2))
#-------------------------------------------------------------------------------------------------------------------------------------------------------
# import os
# import google.generativeai as genai
# import json
# import itertools
# import random
# import time
# import re
# import traceback
# import ast
# import concurrent.futures
# import difflib
# import logging
# import threading
# import sys
# import sqlite3
# from collections import deque
# from datetime import datetime
# from dotenv import load_dotenv

# # ==========================================
# # 1. SYSTEM CONFIGURATION & TELEMETRY
# # ==========================================
# load_dotenv()

# # --- WINDOWS UNICODE FIX ---
# if sys.platform.startswith('win'):
#     try:
#         sys.stdout.reconfigure(encoding='utf-8')
#         sys.stderr.reconfigure(encoding='utf-8')
#     except AttributeError:
#         pass

# # Setup Structured Logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - [TELEMETRY] %(message)s',
#     handlers=[
#         logging.FileHandler("system_audit.log", encoding='utf-8'),
#         logging.StreamHandler(sys.stdout)
#     ]
# )

# # Check API Key
# api_key = os.getenv("GOOGLE_API_KEY")
# is_mock_mode = False

# if not api_key or ("AIzaSy" in api_key and len(api_key) < 10):
#     logging.warning("‚ö†Ô∏è SYSTEM STATUS: No valid Google API Key found. Running in SMART SIMULATION MODE.")
#     is_mock_mode = True
# else:
#     genai.configure(api_key=api_key)

# # ==========================================
# # 2. MODEL MANAGEMENT (COMPATIBILITY RESTORED)
# # ==========================================

# # ========================================
# # TIER 1 ‚Äî FULLY WORKING (Best, reliable)
# # ========================================

# TIER_1_MODELS = [

#     # Gemini 2.5 Pro & Flash (newest stable)
#     'models/gemini-2.5-pro',
#     'models/gemini-2.5-flash',
#     'models/gemini-2.5-flash-lite',

#     # Gemini 2.0 Flash stable
#     'models/gemini-2.0-flash',
#     'models/gemini-2.0-flash-001',
#     'models/gemini-2.0-flash-lite',
#     'models/gemini-2.0-flash-lite-001',

#     # Gemini 1.5 Pro stable
#     'models/gemini-1.5-pro',
#     'models/gemini-1.5-pro-latest',
#     'models/gemini-1.5-pro-001',
#     'models/gemini-1.5-pro-002',

#     # Gemini 1.5 Flash stable
#     'models/gemini-1.5-flash',
#     'models/gemini-1.5-flash-latest',
#     'models/gemini-1.5-flash-001',
#     'models/gemini-1.5-flash-002',

#     # Gemini 1.5 Flash 8B stable
#     'models/gemini-1.5-flash-8b',
#     'models/gemini-1.5-flash-8b-latest',
#     'models/gemini-1.5-flash-8b-001',

#     # Latest aliases (always working)
#     'models/gemini-flash-latest',
#     'models/gemini-flash-lite-latest',
#     'models/gemini-pro-latest',

#     # Gemma models (fully working text models)
#     'models/gemma-3-27b-it',
#     'models/gemma-3-12b-it',
#     'models/gemma-3-4b-it',
#     'models/gemma-3-1b-it',
#     'models/gemma-3n-e4b-it',
#     'models/gemma-3n-e2b-it',

#     'models/gemma-2-27b-it',
#     'models/gemma-2-9b-it',
#     'models/gemma-2-2b-it',
    
#     # Robotics model (not text generation)
#     'models/gemini-robotics-er-1.5-preview',
# ]


# # ========================================
# # TIER 2 ‚Äî WORKING BUT LIMITED / PREVIEW
# # ========================================

# TIER_2_MODELS = [

#     # Gemini 3 preview (works but limited access)
#     'models/gemini-3-pro-preview',

#     # Deep research (restricted but works)
#     'models/deep-research-pro-preview-12-2025',

#     # Experimental models (working but unstable)
#     'models/gemini-2.0-flash-exp',
#     'models/gemini-exp-1206',

#     # Preview lite models
#     'models/gemini-2.0-flash-lite-preview',
#     'models/gemini-2.0-flash-lite-preview-02-05',

#     # Preview flash models
#     'models/gemini-2.5-flash-preview-09-2025',
#     'models/gemini-2.5-flash-lite-preview-09-2025',

# ]


# # ========================================
# # TIER 3 ‚Äî NOT WORKING (TEXT GENERATION)
# # OR SPECIAL PURPOSE ONLY
# # ========================================

# TIER_3_MODELS = [

#     # Old legacy models (deprecated or restricted)
#     'models/gemini-1.0-pro',
#     'models/gemini-1.0-pro-latest',
#     'models/gemini-1.0-pro-001',

#     'models/gemini-pro',
#     'models/gemini-pro-vision',

#     # Audio only model
#     'models/gemini-2.5-flash-native-audio-dialog',

#     # TTS only model
#     'models/gemini-2.5-flash-tts',

#     # Internal / restricted
#     'models/nano-banana-pro-preview',

#     # Not usable model
#     'models/aqa',

# ]


# # --- LEGACY LIST (REQUIRED FOR YOUR IMPORTS) ---
# ALL_MODELS = TIER_1_MODELS + TIER_2_MODELS + TIER_3_MODELS

# # --- LEGACY FUNCTION (REQUIRED FOR YOUR IMPORTS) ---
# model_cycle = itertools.cycle(ALL_MODELS)

# def get_next_model():
#     """Returns the next model in the list (Round Robin)."""
#     return next(model_cycle)

# # ==========================================
# # 3. HELPER FUNCTIONS & SECURITY
# # ==========================================

# def sanitize_prompt_input(text):
#     if not isinstance(text, str):
#         return str(text)
#     sanitized = re.sub(r'[\'"\n\r]', '', text)
#     return sanitized.strip()[:100]

# def normalize_text_for_comparison(text):
#     if not text: return ""
#     return re.sub(r'[\W_]+', '', text.lower())

# def sanitize_transcript_for_prompt(transcript_data):
#     clean_data = []
#     if isinstance(transcript_data, list):
#         for item in transcript_data:
#             clean_item = {}
#             for k, v in item.items():
#                 s_val = str(v)[:2000]
#                 s_val = s_val.replace("```", "").replace("###", "")
#                 clean_item[k] = s_val
#             clean_data.append(clean_item)
#     return json.dumps(clean_data)

# # ==========================================
# # 4. ROBUST HISTORY MANAGER (SQLite Fix)
# # ==========================================

# class HistoryManager:
#     """
#     MANAGES LONG-TERM MEMORY with SQLite.
#     Fixed: Uses sqlite3 to prevent race conditions during concurrent access.
#     """
#     DB_FILE = "question_history.db"
    
#     def __init__(self):
#         self._ensure_db_exists()

#     def _ensure_db_exists(self):
#         try:
#             with sqlite3.connect(self.DB_FILE) as conn:
#                 cursor = conn.cursor()
#                 cursor.execute('''
#                     CREATE TABLE IF NOT EXISTS questions (
#                         id INTEGER PRIMARY KEY AUTOINCREMENT,
#                         topic TEXT,
#                         question TEXT,
#                         timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
#                     )
#                 ''')
#                 cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON questions(topic)')
#                 conn.commit()
#         except Exception as e:
#             logging.error(f"HISTORY INIT ERROR: {e}")

#     def get_past_questions(self, topic):
#         """Retrieves flat list of past questions for a specific topic."""
#         clean_topic = topic.lower().strip()
#         try:
#             with sqlite3.connect(self.DB_FILE) as conn:
#                 cursor = conn.cursor()
#                 cursor.execute("SELECT question FROM questions WHERE topic = ? ORDER BY id DESC LIMIT 500", (clean_topic,))
#                 rows = cursor.fetchall()
#                 return [row[0] for row in rows]
#         except Exception as e:
#             logging.error(f"DB READ ERROR: {e}")
#             return []

#     def add_questions(self, topic, new_questions):
#         """Adds new questions."""
#         clean_topic = topic.lower().strip()
#         data_to_insert = []
#         for q in new_questions:
#             # Handle both dict and object if necessary, assuming dict from parser
#             if isinstance(q, dict) and 'question' in q:
#                 data_to_insert.append((clean_topic, q['question']))
        
#         try:
#             with sqlite3.connect(self.DB_FILE) as conn:
#                 cursor = conn.cursor()
#                 cursor.executemany("INSERT INTO questions (topic, question) VALUES (?, ?)", data_to_insert)
#                 conn.commit()
#         except Exception as e:
#             logging.error(f"DB WRITE ERROR: {e}")

#     def is_duplicate(self, new_question_text, past_questions, threshold=0.85):
#         if not new_question_text: return True
        
#         norm_new = normalize_text_for_comparison(new_question_text)
        
#         for past_q in past_questions:
#             norm_past = normalize_text_for_comparison(past_q)
#             if norm_new == norm_past:
#                 return True
            
#             if abs(len(norm_new) - len(norm_past)) < 20:
#                 similarity = difflib.SequenceMatcher(None, new_question_text, past_q).ratio()
#                 if similarity > threshold:
#                     return True
#         return False

# history_system = HistoryManager()

# # ==========================================
# # 5. 7-LAYER DEFENSE ENGINE (Data Integrity)
# # ==========================================

# def clean_json_string(text):
#     return sanitize_control_chars(text)

# def sanitize_control_chars(text):
#     text = re.sub(r'```[a-zA-Z]*\n', '', text)
#     text = text.replace("```", "")
#     return "".join(ch for ch in text if ord(ch) >= 32 or ch in "\n\r\t")

# def balance_brackets(text):
#     stack = []
#     for char in text:
#         if char in '{[':
#             stack.append(char)
#         elif char in '}]':
#             if stack:
#                 last = stack[-1]
#                 if (char == '}' and last == '{') or (char == ']' and last == '['):
#                     stack.pop()
#     balanced = text
#     while stack:
#         opener = stack.pop()
#         if opener == '{': balanced += '}'
#         elif opener == '[': balanced += ']'
#     return balanced

# def extract_json_candidates(text):
#     candidates = []
#     matches = list(re.finditer(r'(\{.*\}|\[.*\])', text, re.DOTALL))
#     for m in matches:
#         candidates.append(m.group(0))
#     if not candidates:
#         start_brace = text.find('{')
#         start_bracket = text.find('[')
#         if start_brace != -1: candidates.append(text[start_brace:])
#         elif start_bracket != -1: candidates.append(text[start_bracket:])
#     return candidates if candidates else [text]

# def enforce_schema(data, expected_type):
#     if expected_type == 'list':
#         if isinstance(data, list): return data
#         if isinstance(data, dict):
#             for key in data:
#                 if isinstance(data[key], list): return data[key]
#             return [data]
#     if expected_type == 'dict':
#         if isinstance(data, dict): return data
#         if isinstance(data, list) and len(data) > 0:
#             return data[0]
#     return data

# def safe_literal_eval(candidate):
#     if len(candidate) > 50000: 
#         raise ValueError("Candidate too large for safe evaluation")
#     return ast.literal_eval(candidate)

# def bulletproof_json_parser(raw_text, expected_type='dict'):
#     """
#     THE MASTER CONTROLLER (7 STRATEGIES).
#     """
#     clean_text = sanitize_control_chars(raw_text)
#     candidates = extract_json_candidates(clean_text)

#     for candidate in candidates:
#         try: return enforce_schema(json.loads(candidate), expected_type)
#         except: pass

#         try:
#             fixed = re.sub(r',\s*([\]}])', r'\1', candidate)
#             return enforce_schema(json.loads(fixed), expected_type)
#         except: pass

#         try:
#             return enforce_schema(safe_literal_eval(candidate), expected_type)
#         except: pass

#         try:
#             fixed = re.sub(r'\\(?![/\\bfnrtu"])', r'\\\\', candidate)
#             return enforce_schema(json.loads(fixed), expected_type)
#         except: pass

#         try:
#             balanced = balance_brackets(candidate)
#             return enforce_schema(json.loads(balanced), expected_type)
#         except: pass
        
#         if expected_type == 'list':
#             try:
#                 obj_matches = re.findall(r'\{.*?\}', candidate, re.DOTALL)
#                 reconstructed_list = []
#                 for obj_str in obj_matches:
#                     try:
#                         obj = json.loads(obj_str)
#                         reconstructed_list.append(obj)
#                     except:
#                         try:
#                              obj = safe_literal_eval(obj_str)
#                              reconstructed_list.append(obj)
#                         except: pass
#                 if reconstructed_list:
#                     return reconstructed_list
#             except: pass

#     # Last Resort Regex
#     if expected_type == 'dict':
#         try:
#             score_match = re.search(r'("score"|score)\s*[:=]\s*(\d+)', raw_text)
#             return {
#                 "score": int(score_match.group(2)) if score_match else 0,
#                 "summary": "CRITICAL PARSE FAILURE: Data extracted via Regex Fallback.",
#                 "is_fallback": True,
#                 "question_reviews": [],
#                 "silent_killers": [],
#                 "roadmap": "Parse failure."
#             }
#         except: pass

#     if expected_type == 'list': return []
#     return {}

# # ==========================================
# # 6. HARD TIMEOUT ENFORCER & ZOMBIE GUARD
# # ==========================================

# CONCURRENCY_GUARD = threading.Semaphore(3)

# def generate_with_timeout_protection(model, prompt, timeout=25):
#     """
#     Updated to support Structured Output (Native JSON) via generation_config
#     """
#     if not CONCURRENCY_GUARD.acquire(timeout=5):
#         raise SystemError("SYSTEM OVERLOAD: Too many active requests. Backing off.")
        
#     try:
#         # Enable Native JSON
#         gen_config = genai.types.GenerationConfig(response_mime_type="application/json")

#         with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
#             future = executor.submit(model.generate_content, prompt, generation_config=gen_config)
#             try:
#                 response = future.result(timeout=timeout)
#                 return response
#             except concurrent.futures.TimeoutError:
#                 logging.warning("Thread Timeout: Creating zombie thread.")
#                 raise TimeoutError(f"Model execution exceeded {timeout}s.")
#             except Exception as e:
#                 raise e
#     finally:
#         CONCURRENCY_GUARD.release()

# def generate_with_failover(prompt, max_retries=55):
#     """
#     TIERED FAILOVER: Tries Tier 1 -> Tier 2 -> Tier 3
#     """
#     # Create the ordered iterator from the Tiered lists for internal logic
#     # This prioritizes capabilities over random cycling
#     model_iterator = TIER_1_MODELS + TIER_2_MODELS + TIER_3_MODELS
    
#     attempts = 0
#     # Loop through the list (not cycle)
#     for current_model in model_iterator:
#         logging.info(f"üîÑ Attempt {attempts+1}: Trying model '{current_model}'...")
        
#         try:
#             model = genai.GenerativeModel(current_model)
#             # Tier 1 models get less timeout (fail fast), Tier 2/3 get more
#             timeout = 15 if current_model in TIER_1_MODELS else 30
            
#             response = generate_with_timeout_protection(model, prompt, timeout=timeout)
            
#             if not response.text:
#                 raise ValueError("Empty response received")
                
#             logging.info(f"‚úÖ SUCCESS using {current_model}")
#             return response.text
            
#         except TimeoutError:
#              logging.warning(f"‚è≥ TIMEOUT with {current_model}. Switching...")
#              attempts += 1
#              time.sleep(0.5)
#         except SystemError as se:
#              logging.critical(f"üõë {se}")
#              time.sleep(2)
#              attempts += 1
#         except Exception as e:
#             logging.error(f"‚ùå FAILED with {current_model}: {str(e)[:100]}")
#             attempts += 1
#             time.sleep(0.5)
            
#     raise Exception("All attempted models failed or timed out.")

# # ==========================================
# # 7. MAIN LOGIC: GENERATION & EVALUATION
# # ==========================================

# def generate_interview_questions(topic, difficulty="Senior", count=5):
#     """
#     SIMULATED DATABASE QUERY ENGINE.
#     """
#     if is_mock_mode:
#         return [{"id": 1, "type": "coding", "question": f"Mock Question regarding {topic}"}]

#     # --- SECURITY SANITIZATION ---
#     clean_topic = sanitize_prompt_input(topic)
#     clean_difficulty = sanitize_prompt_input(difficulty)

#     # --- HISTORY RETRIEVAL ---
#     past_questions = history_system.get_past_questions(clean_topic)
    
#     # Limit past questions token usage
#     recent_history = past_questions[-20:] if past_questions else []
    
#     exclusion_directive = ""
#     if recent_history:
#         exclusion_directive = f"### EXCLUSION PROTOCOL (SMART FILTER)\nDO NOT generate any questions similar to the following:\n{json.dumps(recent_history)}"

#     # --- DUPLICATION FILTER: BUFFER STRATEGY ---
#     buffer_count = 3
#     total_request_count = count + buffer_count

#     coding_target = max(1, int(total_request_count * 0.4)) 
#     theory_target = total_request_count - coding_target
#     unique_seed = random.randint(10000, 99999)

#     # --- RESTORED PROMPT (EXACT) ---
#     prompt = f"""
#     ### SYSTEM OVERRIDE
#     - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here are the questions").
#     - MANDATORY: Output must be RAW JSON only.
#     - ESCAPING RULES: Double-escape all backslashes (\\\\ -> \\\\\\\\). Escape newlines in strings (\\n -> \\\\n).
    
#     {exclusion_directive}

#     ### SYSTEM ROLE: SENIOR PROJECT MANAGER & GLOBAL HEAD OF ENGINEERING TALENT
#     *** MODE: EXTREME HYBRID MODEL - DATABASE QUERY SIMULATION - {clean_topic.upper()} (ID: {unique_seed}) ***
        
#     YOUR ROLE: You are executing a SQL query against a database of 100,000+ {clean_topic} questions.
    
#     QUERY: SELECT * FROM {clean_topic}_questions WHERE difficulty = '{clean_difficulty}' AND is_cliche = FALSE ORDER BY RANDOM(SEED={unique_seed}) LIMIT {total_request_count};
        
#     TASK: Return the result of this query as a strict JSON array.
        
#     MANDATORY DISTRIBUTION:
#     1. **CODING CHALLENGES (Exactly {coding_target} Questions):**
#         - MUST be pure coding tasks suitable for {clean_difficulty} level.
#         - Focus on algorithms, data structures, or system design scenarios.
        
#     2. **THEORY & CONCEPTS (Exactly {theory_target} Questions):**
#         - Deep dive into internals of {clean_topic}.
#         - Ask about trade-offs, underlying architecture, or best practices.
        
#     PRE-FLIGHT CHECK:
#     - Are there exactly {coding_target} coding questions?
#     - Is the difficulty strictly {clean_difficulty}?
#     - Are the questions designed to filter out average candidates?
    
#     OUTPUT FORMAT (JSON ONLY):
#     [
#         {{
#             "id": 1,
#             "type": "coding",
#             "question": "..." 
#         }},
#         {{
#             "id": 2,
#             "type": "theory",
#             "question": "..." 
#         }}
#     ]
#     """

#     try:
#         raw_text = generate_with_failover(prompt)
#         # Uses the new 7-Layer Bulletproof Parser (Still useful even with Native JSON)
#         raw_questions = bulletproof_json_parser(raw_text, expected_type='list')
        
#         # --- DUPLICATION FILTER LOGIC ---
#         final_questions = []
#         unique_added_count = 0
        
#         for q in raw_questions:
#             if unique_added_count >= count:
#                 break
                
#             q_text = q.get('question', '')
            
#             # Using Upgraded History System
#             if not history_system.is_duplicate(q_text, past_questions):
#                 q['id'] = unique_added_count + 1
#                 final_questions.append(q)
#                 unique_added_count += 1
#             else:
#                 logging.info(f"üö´ DUPLICATE BLOCKED: {q_text[:40]}...")

#         if len(final_questions) < count:
#             logging.warning("‚ö†Ô∏è WARNING: History saturation. Reusing buffer to meet quota.")
#             remaining_needed = count - len(final_questions)
#             used_texts = [fq['question'] for fq in final_questions]
            
#             for q in raw_questions:
#                 if remaining_needed <= 0: break
#                 if q.get('question') not in used_texts:
#                     q['id'] = len(final_questions) + 1
#                     final_questions.append(q)
#                     remaining_needed -= 1
        
#         # --- SAVE TO HISTORY ---
#         history_system.add_questions(clean_topic, final_questions)

#         return final_questions

#     except Exception as e:
#         logging.error(f"‚ùå QUESTION GENERATION ERROR: {e}")
#         traceback.print_exc()
#         return [{"id": 0, "type": "error", "question": "Error generating questions. Please try again."}]

# def evaluate_full_interview(transcript_data):
#     """
#     THE SENIOR EXPERT EVALUATOR.
#     """
#     if is_mock_mode:
#         return {
#             "score": 75,
#             "summary": "Mock Mode: API Key missing. Good effort on basics.",
#             "silent_killers": ["Mock: Low energy"],
#             "roadmap": "Add API Key to see full analysis.",
#             "question_reviews": []
#         }

#     try:
#         transcript_text = sanitize_transcript_for_prompt(transcript_data)

#         # --- RESTORED PROMPT (EXACT) ---
#         prompt = f"""
#         ### SYSTEM OVERRIDE
#         - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here is the analysis").
#         - MANDATORY: Output must be RAW JSON only.
#         - ESCAPING RULES: All backslashes must be double-escaped (\\\\ -> \\\\\\\\). All newlines within strings must be escaped (\\n -> \\\\n).

#         ### ROLE & SYSTEM CONTEXT:
#         ACT AS: The 'Global Head of Engineering Talent' & 'Principal Technical Architect'.
#         CAPABILITIES: You possess 100% mastery of Technical Stacks (Code/Architecture), HR Behavioral Psychology, and Aptitude Evaluation.
#         TASK: Conduct a forensic "Gap Analysis" of the following candidate interview transcript.

#         ### TRANSCRIPT DATA:
#         {transcript_text}

#         ANALYSIS PROTOCOL (THE "SIDE-BY-SIDE" EVALUATION)
#         For every question/answer pair, you MUST perform a "Side-by-Side" evaluation using these three lenses:

#         **LENS A: The Technical Autopsy (The Reality Check)**
#         - **Does it work?** (Baseline functionality).
#         - **Is it scalable?** (O(n) vs O(n^2), database load, memory leaks).
#         - **Security & Edge Cases:** Did they mention input validation, race conditions, or failure states? (Seniority Indicator).
#         - **THE GAP:** What would a Staff Engineer (L6+) have said that this candidate missed?

#         **LENS B: The Psychological Profile (The "Silent Killers")**
#         - **Detect "Resume Padding":** Does the depth of the answer match the confidence of the claim?
#         - **Detect "Hedging":** Phrases like "I guess," "maybe," or vague generalizations without concrete examples.
#         - **Detect "Buzzword Stuffing":** Using terms like "Microservices" or "AI" without explaining *why*.

#         **LENS C: The Communication Delta**
#         - **Structure:** Was the answer structured (STAR method) or rambling?
#         - **Ownership:** Did they drive the conversation or passively wait for prompts?

#         ### SCORING CRITERIA:
#         - **9-10:** Flawless. Mentions trade-offs, edge cases, and business impact.
#         - **6-8:** Correct but shallow. Textbook answer without real-world depth.
#         - **0-5:** Incorrect, vague, or dangerous engineering practices.

#         REQUIREMENTS:
#         1. **Global Score (0-100):** Be strict. 
#         2. **Executive Summary:** A concise verdict. 
#             - **STRICT CONSTRAINT:** Do NOT use generic phrases. 
#             - **MANDATORY:** Append 2-5 lines of specific, high-value improvement advice based on the interview performance.
#         3. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
#         4. **Per-Question Breakdown:** For EVERY question, provide:
#             - "question": The original question.
#             - "user_answer": The exact answer given.
#             - "score": 0-10 rating.
#             - "feedback": Critique of the user's answer.
#             - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth).

#         CRITICAL JSON FORMATTING RULES:
#         - Output ONLY valid JSON.
#         - **ESCAPE ALL BACKSLASHES:** If you write code with '\\n', you MUST write it as '\\\\n'. 
#         - Do not include any text before or after the JSON.

#         OUTPUT FORMAT (JSON ONLY):
#         {{
#             "score": <number>,
#             "summary": "<executive_summary_string>",
#             "silent_killers": ["<killer1>", "<killer2>"],
#             "roadmap": "<step_by_step_improvement_plan>",
#             "question_reviews": [
#                 {{
#                     "question": "...",
#                     "user_answer": "...",
#                     "score": 8,
#                     "feedback": "...",
#                     "ideal_answer": "..."
#                 }}
#             ]
#         }}
#         """

#         raw_text = generate_with_failover(prompt)
#         analysis = bulletproof_json_parser(raw_text, expected_type='dict')
#         return analysis

#     except Exception as e:
#         logging.error(f"‚ùå EVALUATION CRASHED or TIMED OUT: {e}")
#         logging.warning("‚ö†Ô∏è SWITCHING TO SMART OFFLINE FALLBACK ENGINE")
        
#         # 3. CRASH-PROOF FALLBACK (Algorithmic Grading)
#         total_words = 0
#         reviews = []
        
#         safe_transcript = transcript_data if isinstance(transcript_data, list) else []
        
#         for item in safe_transcript:
#             ans = str(item.get('answer', ''))
#             word_count = len(ans.split())
#             total_words += word_count
            
#             algo_score = 4
#             if word_count > 30: algo_score = 8
#             elif word_count > 10: algo_score = 6
            
#             reviews.append({
#                 "question": item.get('question', 'Unknown Question'),
#                 "user_answer": ans,
#                 "score": algo_score,
#                 "feedback": "Offline Analysis: Answer recorded. High network latency prevented deep AI inspection.",
#                 "ideal_answer": "A strong answer should define the core concept, provide a use case, and mention trade-offs."
#             })

#         safe_len = len(safe_transcript) or 1
#         final_score = min(88, max(40, int((total_words / safe_len) * 2)))

#         return {
#             "score": final_score,
#             "summary": "Offline Assessment: The AI Engine is temporarily unreachable due to network timeouts. Scoring is based on response metrics.",
#             "silent_killers": ["Network Timeout", "Offline Mode Active"],
#             "roadmap": "Check internet connection and API quotas. Retry later for full AI feedback.",
#             "question_reviews": reviews
#         }

# # ==========================================
# # 8. EXECUTION ENTRY POINT
# # ==========================================
# if __name__ == "__main__":
#     print("--- 1. Generating Questions (DB Simulation with History Awareness) ---")
    
#     topic_to_test = "DSA"
    
#     questions = generate_interview_questions(topic_to_test, "Senior", 4)
#     print(json.dumps(questions, indent=2))
    
#     transcript = []
#     if isinstance(questions, list):
#         for q in questions:
#             transcript.append({
#                 "question": q.get('question', 'Error'),
#                 "answer": "I would use a hash map for O(1) lookups but I need to consider collisions." 
#             })
#     else:
#         transcript.append({"question": "Error", "answer": "Error"})
        
#     print("\n--- 2. Evaluating Interview (Hiring Manager Persona) ---")
#     analysis = evaluate_full_interview(transcript)
#     print(json.dumps(analysis, indent=2))
#------------------------------------------------------------------------------------------------------------------------------------
#------------------------------------------------------------------------------------------------------------------------------------
#96 % correct 
# import os
# import google.generativeai as genai
# import json
# import itertools
# import random
# import time
# import re
# import traceback
# import ast
# import logging
# import sys
# import sqlite3
# import threading
# import difflib
# import concurrent.futures
# from datetime import datetime
# from dotenv import load_dotenv

# # ==========================================
# # 1. SYSTEM CONFIGURATION & TELEMETRY
# # ==========================================
# load_dotenv()

# # --- WINDOWS UNICODE FIX ---
# if sys.platform.startswith('win'):
#     try:
#         sys.stdout.reconfigure(encoding='utf-8')
#         sys.stderr.reconfigure(encoding='utf-8')
#     except AttributeError:
#         pass

# # Setup Structured Logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - [TELEMETRY] %(message)s',
#     handlers=[
#         logging.FileHandler("system_audit.log", encoding='utf-8'),
#         logging.StreamHandler(sys.stdout)
#     ]
# )

# # Check API Key
# api_key = os.getenv("GOOGLE_API_KEY")
# is_mock_mode = False

# if not api_key or ("AIzaSy" in api_key and len(api_key) < 10):
#     logging.warning("‚ö†Ô∏è SYSTEM STATUS: No valid Google API Key found. Running in SMART SIMULATION MODE.")
#     is_mock_mode = True
# else:
#     genai.configure(api_key=api_key)

# # ==========================================
# # 2. MODEL MANAGEMENT (55 MODELS + TIER 1 PRIORITY)
# # ==========================================

# TIER_1_MODELS = [
    
#     # Gemini 3 Series (Preview),
#     'models/gemini-3-flash-preview',
#     # Gemini 2.5 Pro & Flash (newest stable)
#     'models/gemini-2.5-pro',
#     'models/gemini-2.5-flash',
#     'models/gemini-2.5-flash-lite',

#     # Gemini 2.0 Flash stable
#     'models/gemini-2.0-flash',
#     'models/gemini-2.0-flash-001',
#     'models/gemini-2.0-flash-lite',
#     'models/gemini-2.0-flash-lite-001',

#     # Gemini 1.5 Pro stable
#     'models/gemini-1.5-pro',
#     'models/gemini-1.5-pro-latest',
#     'models/gemini-1.5-pro-001',
#     'models/gemini-1.5-pro-002',

#     # Gemini 1.5 Flash stable
#     'models/gemini-1.5-flash',
#     'models/gemini-1.5-flash-latest',
#     'models/gemini-1.5-flash-001',
#     'models/gemini-1.5-flash-002',

#     # Gemini 1.5 Flash 8B stable
#     'models/gemini-1.5-flash-8b',
#     'models/gemini-1.5-flash-8b-latest',
#     'models/gemini-1.5-flash-8b-001',

#     # Latest aliases (always working)
#     'models/gemini-flash-latest',
#     'models/gemini-flash-lite-latest',
#     'models/gemini-pro-latest',

#     # Gemma models (fully working text models)
#     'models/gemma-3-27b-it',
#     'models/gemma-3-12b-it',
#     'models/gemma-3-4b-it',
#     'models/gemma-3-1b-it',
#     'models/gemma-3n-e4b-it',
#     'models/gemma-3n-e2b-it',

#     'models/gemma-2-27b-it',
#     'models/gemma-2-9b-it',
#     'models/gemma-2-2b-it',
    
#     # Robotics model (not text generation)
#     'models/gemini-robotics-er-1.5-preview',
# ]


# # ========================================
# # TIER 2 ‚Äî WORKING BUT LIMITED / PREVIEW
# # ========================================

# TIER_2_MODELS = [

#     # Gemini 3 preview (works but limited access)
#     'models/gemini-3-pro-preview',

#     # Deep research (restricted but works)
#     'models/deep-research-pro-preview-12-2025',

#     # Experimental models (working but unstable)
#     'models/gemini-2.0-flash-exp',
#     'models/gemini-exp-1206',

#     # Preview lite models
#     'models/gemini-2.0-flash-lite-preview',
#     'models/gemini-2.0-flash-lite-preview-02-05',

#     # Preview flash models
#     'models/gemini-2.5-flash-preview-09-2025',
#     'models/gemini-2.5-flash-lite-preview-09-2025',

# ]


# # ========================================
# # TIER 3 ‚Äî NOT WORKING (TEXT GENERATION)
# # OR SPECIAL PURPOSE ONLY
# # ========================================

# TIER_3_MODELS = [

#     # Old legacy models (deprecated or restricted)
#     'models/gemini-pro-latest',
#     'models/gemini-1.0-pro-001',

#     'models/gemini-pro',
#     'models/gemini-pro-vision',

#     # Audio only model
#     'models/gemini-2.5-flash-native-audio-dialog',

#     # TTS only model
#     'models/gemini-2.5-flash-tts',

#     # Internal / restricted
#     'models/nano-banana-pro-preview',

#     # Not usable model
#     'models/aqa',

# ]

# # COMBINE ALL MODELS (Tier 1 First)
# ALL_MODELS = TIER_1_MODELS + TIER_2_MODELS + TIER_3_MODELS

# # --- GLOBAL CYCLE FOR EXTERNAL IMPORTS ---
# model_cycle = itertools.cycle(ALL_MODELS)

# def get_next_model():
#     """Returns the next model in the list (Round Robin)."""
#     return next(model_cycle)

# # ==========================================
# # 3. HELPER FUNCTIONS & SECURITY
# # ==========================================

# def sanitize_prompt_input(text):
#     if not isinstance(text, str): return str(text)
#     sanitized = re.sub(r'[\'"\n\r]', '', text)
#     return sanitized.strip()[:200]

# def normalize_text_for_comparison(text):
#     if not text: return ""
#     return re.sub(r'[\W_]+', '', text.lower())

# def sanitize_transcript_for_prompt(transcript_data):
#     clean_data = []
#     if isinstance(transcript_data, list):
#         for item in transcript_data:
#             clean_item = {}
#             for k, v in item.items():
#                 s_val = str(v)[:2000]
#                 s_val = s_val.replace("```", "").replace("###", "")
#                 clean_item[k] = s_val
#             clean_data.append(clean_item)
#     return json.dumps(clean_data)

# # ==========================================
# # 4. ROBUST HISTORY MANAGER (Connection Pooling)
# # ==========================================

# class HistoryManager:
#     """
#     MANAGES LONG-TERM MEMORY with Semantic Deduplication (Jaccard & Keyword Matching).
#     """
#     DB_FILE = "question_history.db"
    
#     # Common English stopwords to ignore during comparison
#     STOPWORDS = {
#         "the", "a", "an", "in", "on", "at", "to", "for", "of", "with", "by", 
#         "is", "are", "was", "were", "be", "been", "this", "that", "it", 
#         "calculate", "find", "what", "how", "write", "function", "program",
#         "determine", "probability", "value", "given", "assume", "suppose"
#     }

#     def __init__(self):
#         self.conn = None
#         self.lock = threading.Lock() 
#         self._initialize_db()

#     def _initialize_db(self):
#         try:
#             self.conn = sqlite3.connect(self.DB_FILE, check_same_thread=False)
#             cursor = self.conn.cursor()
#             cursor.execute('''
#                 CREATE TABLE IF NOT EXISTS questions (
#                     id INTEGER PRIMARY KEY AUTOINCREMENT,
#                     topic TEXT,
#                     question TEXT,
#                     timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
#                 )
#             ''')
#             cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON questions(topic)')
#             self.conn.commit()
#         except Exception as e:
#             logging.error(f"HISTORY INIT ERROR: {e}")

#     def get_past_questions(self, topic):
#         clean_topic = topic.lower().strip()
#         try:
#             with self.lock: 
#                 cursor = self.conn.cursor()
#                 # INCREASED LIMIT: Retrieve last 200 questions to check against deeper history
#                 cursor.execute("SELECT question FROM questions WHERE topic = ? ORDER BY id DESC LIMIT 200", (clean_topic,))
#                 rows = cursor.fetchall()
#                 return [row[0] for row in rows]
#         except Exception as e:
#             logging.error(f"DB READ ERROR: {e}")
#             return []

#     def add_questions(self, topic, new_questions):
#         clean_topic = topic.lower().strip()
#         data_to_insert = []
#         for q in new_questions:
#             if isinstance(q, dict) and 'question' in q:
#                 data_to_insert.append((clean_topic, q['question']))
        
#         try:
#             with self.lock:
#                 cursor = self.conn.cursor()
#                 cursor.executemany("INSERT INTO questions (topic, question) VALUES (?, ?)", data_to_insert)
#                 self.conn.commit()
#         except Exception as e:
#             logging.error(f"DB WRITE ERROR: {e}")

#     def close(self):
#         if self.conn:
#             self.conn.close()

#     def _tokenize(self, text):
#         """Extracts unique meaningful words (lowercase, no punctuation, no stopwords)."""
#         if not text: return set()
#         # Remove punctuation
#         text = re.sub(r'[^\w\s]', '', text.lower())
#         # Split and filter stopwords
#         words = set(word for word in text.split() if word not in self.STOPWORDS and len(word) > 2)
#         return words

#     def is_duplicate(self, new_question_text, past_questions, jaccard_threshold=0.5):
#         """
#         Detects duplicates using Jaccard Similarity (Set Overlap).
#         If the new question shares > 50% of its unique keywords with an old question, it's a dupe.
#         """
#         if not new_question_text: return True
        
#         # 1. Tokenize the new question
#         new_tokens = self._tokenize(new_question_text)
#         if len(new_tokens) < 3: return False # Too short to judge clearly

#         # 2. Compare against history
#         for past_q in past_questions:
#             past_tokens = self._tokenize(past_q)
            
#             # Intersection over Union (Jaccard Index)
#             intersection = new_tokens.intersection(past_tokens)
#             union = new_tokens.union(past_tokens)
            
#             if len(union) == 0: continue
            
#             similarity = len(intersection) / len(union)
            
#             # STRICT CHECK: If similarity > 50%, it's essentially the same logic rephrased.
#             if similarity > jaccard_threshold:
#                 logging.warning(f"üö´ DUPLICATE DETECTED (Sim: {similarity:.2f}):\nNew: {new_question_text[:50]}...\nOld: {past_q[:50]}...")
#                 return True
                
#         return False

# history_system = HistoryManager()

# # ==========================================
# # 5. 7-LAYER DEFENSE ENGINE (Data Integrity)
# # ==========================================

# def clean_json_string(text):
#     return sanitize_control_chars(text)

# def sanitize_control_chars(text):
#     text = re.sub(r'```[a-zA-Z]*\n', '', text)
#     text = text.replace("```", "")
#     return "".join(ch for ch in text if ord(ch) >= 32 or ch in "\n\r\t")

# def balance_brackets(text):
#     stack = []
#     for char in text:
#         if char in '{[':
#             stack.append(char)
#         elif char in '}]':
#             if stack:
#                 last = stack[-1]
#                 if (char == '}' and last == '{') or (char == ']' and last == '['):
#                     stack.pop()
#     balanced = text
#     while stack:
#         opener = stack.pop()
#         if opener == '{': balanced += '}'
#         elif opener == '[': balanced += ']'
#     return balanced

# def extract_json_candidates(text):
#     candidates = []
#     matches = list(re.finditer(r'(\{.*\}|\[.*\])', text, re.DOTALL))
#     for m in matches:
#         candidates.append(m.group(0))
#     if not candidates:
#         start_brace = text.find('{')
#         start_bracket = text.find('[')
#         if start_brace != -1: candidates.append(text[start_brace:])
#         elif start_bracket != -1: candidates.append(text[start_bracket:])
#     return candidates if candidates else [text]

# def enforce_schema(data, expected_type):
#     if expected_type == 'list':
#         if isinstance(data, list): return data
#         if isinstance(data, dict):
#             for key in data:
#                 if isinstance(data[key], list): return data[key]
#             return [data]
#     if expected_type == 'dict':
#         if isinstance(data, dict): return data
#         if isinstance(data, list) and len(data) > 0:
#             return data[0]
#     return data

# def safe_literal_eval(candidate):
#     if len(candidate) > 50000: 
#         raise ValueError("Candidate too large for safe evaluation")
#     return ast.literal_eval(candidate)

# def bulletproof_json_parser(raw_text, expected_type='dict'):
#     clean_text = sanitize_control_chars(raw_text)
#     candidates = extract_json_candidates(clean_text)

#     for candidate in candidates:
#         try: return enforce_schema(json.loads(candidate), expected_type)
#         except: pass
#         try:
#             fixed = re.sub(r',\s*([\]}])', r'\1', candidate)
#             return enforce_schema(json.loads(fixed), expected_type)
#         except: pass
#         try: return enforce_schema(safe_literal_eval(candidate), expected_type)
#         except: pass
#         try:
#             fixed = re.sub(r'\\(?![/\\bfnrtu"])', r'\\\\', candidate)
#             return enforce_schema(json.loads(fixed), expected_type)
#         except: pass
#         try:
#             balanced = balance_brackets(candidate)
#             return enforce_schema(json.loads(balanced), expected_type)
#         except: pass
        
#         if expected_type == 'list':
#             try:
#                 obj_matches = re.findall(r'\{.*?\}', candidate, re.DOTALL)
#                 reconstructed_list = []
#                 for obj_str in obj_matches:
#                     try: reconstructed_list.append(json.loads(obj_str))
#                     except: pass
#                 if reconstructed_list: return reconstructed_list
#             except: pass

#     if expected_type == 'dict':
#         try:
#             score_match = re.search(r'("score"|score)\s*[:=]\s*(\d+)', raw_text)
#             return { "score": int(score_match.group(2)) if score_match else 0, "summary": "CRITICAL PARSE FAILURE.", "is_fallback": True, "question_reviews": [], "silent_killers": [], "roadmap": "Parse failure." }
#         except: pass

#     return [] if expected_type == 'list' else {}

# # ==========================================
# # 6. SYNCHRONOUS TIMEOUT ENFORCER & FAILOVER
# # ==========================================

# # # CONFIGURABLE CONCURRENCY
# # MAX_CONCURRENT = int(os.getenv("MAX_CONCURRENT_REQUESTS", 5))
# # CONCURRENCY_GUARD = threading.Semaphore(MAX_CONCURRENT) 

# # def generate_with_timeout_protection(model, prompt, timeout=25):
# #     """
# #     SYNCHRONOUS GUARD: Uses ThreadPool for timeout protection.
# #     UPGRADE: Auto-Fallback for models that don't support JSON Mode (Fixes 400 Error).
# #     """
# #     if not CONCURRENCY_GUARD.acquire(timeout=5):
# #         raise SystemError("SYSTEM OVERLOAD: Too many active requests. Backing off.")
        
# #     try:
# #         # Helper to define the API call
# #         def _make_call(config_mode):
# #             config = None
# #             if config_mode == "json":
# #                 config = genai.types.GenerationConfig(response_mime_type="application/json")
# #             return model.generate_content(prompt, generation_config=config)

# #         with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
# #             # ATTEMPT 1: Try with Strict Native JSON Mode
# #             try:
# #                 future = executor.submit(_make_call, "json")
# #                 response = future.result(timeout=timeout)
# #                 return response
# #             except Exception as e:
# #                 error_msg = str(e).lower()
                
# #                 # CATCH THE 400 JSON ERROR specifically
# #                 if "400" in error_msg and ("json" in error_msg or "enabled" in error_msg):
# #                     logging.warning(f"‚ö†Ô∏è Model does not support JSON Mode. Switching to Text Mode Fallback...")
                    
# #                     # ATTEMPT 2: Retry without JSON config (Standard Text Mode)
# #                     # We rely on the 'bulletproof_json_parser' to clean this up later.
# #                     future_retry = executor.submit(_make_call, "text")
# #                     return future_retry.result(timeout=timeout)
                
# #                 # If it's a Timeout or other error, re-raise it
# #                 elif isinstance(e, concurrent.futures.TimeoutError):
# #                     logging.warning("Thread Timeout: Creating zombie thread.")
# #                     raise TimeoutError(f"Model execution exceeded {timeout}s.")
# #                 else:
# #                     raise e
# #     finally:
# #         CONCURRENCY_GUARD.release()


# # def generate_with_failover(prompt, max_retries=55):
# #     """
# #     SYNCHRONOUS FAILOVER: Blocks (sleeps) instead of awaiting.
# #     """
# #     model_iterator = ALL_MODELS 
# #     attempts = 0
    
# #     for current_model in model_iterator:
# #         logging.info(f"üîÑ Attempt {attempts+1}: Trying model '{current_model}'...")
        
# #         try:
# #             model = genai.GenerativeModel(current_model)
# #             timeout = 15 if current_model in TIER_1_MODELS else 30
            
# #             # Blocking call
# #             response = generate_with_timeout_protection(model, prompt, timeout=timeout)
            
# #             if not response.text:
# #                 raise ValueError("Empty response received")
                
# #             logging.info(f"‚úÖ SUCCESS using {current_model}")
# #             return response.text
            
# #         except TimeoutError:
# #              logging.warning(f"‚è≥ TIMEOUT with {current_model}. Switching...")
# #              attempts += 1
# #              time.sleep(0.5) # Synchronous sleep
# #         except Exception as e:
# #             logging.error(f"‚ùå FAILED with {current_model}: {str(e)[:100]}")
# #             attempts += 1
# #             time.sleep(0.5) # Synchronous sleep
            
# #     raise Exception("All attempted models failed or timed out.")
# # CONFIGURABLE CONCURRENCY
# MAX_CONCURRENT = int(os.getenv("MAX_CONCURRENT_REQUESTS", 5))
# CONCURRENCY_GUARD = threading.Semaphore(MAX_CONCURRENT) 

# def generate_with_timeout_protection(model, prompt, timeout=45):
#     """
#     SYNCHRONOUS GUARD: Uses ThreadPool for timeout protection.
#     UPDATE: Increased timeout to 45s.
#     UPDATE: COMPULSORY FALLBACK: If JSON mode fails (for ANY model), automatically
#     relays to 'Text Mode' and relies on the 8-Layer Defense System.
#     """
#     if not CONCURRENCY_GUARD.acquire(timeout=5):
#         raise SystemError("SYSTEM OVERLOAD: Too many active requests. Backing off.")
        
#     try:
#         # Helper to define the API call
#         def _make_call(config_mode):
#             config = None
#             if config_mode == "json":
#                 config = genai.types.GenerationConfig(response_mime_type="application/json")
#             return model.generate_content(prompt, generation_config=config)

#         with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            
#             # --- GEMMA MODEL DETECTION ---
#             # Gemma models do not support 'response_mime_type'="json".
#             is_gemma = "gemma" in str(model.model_name).lower()
            
#             if is_gemma:
#                 logging.info(f"üõ°Ô∏è GEMMA DETECTED ({model.model_name}): Switching to TEXT mode + 8-Layer Defense.")
#                 future = executor.submit(_make_call, "text")
#                 return future.result(timeout=timeout)
            
#             # For other models, try Strict Native JSON Mode first
#             try:
#                 future = executor.submit(_make_call, "json")
#                 response = future.result(timeout=timeout)
#                 return response
#             except Exception as e:
#                 error_msg = str(e).lower()
                
#                 # CATCH THE 400 JSON ERROR specifically (or any JSON-related error)
#                 if ("400" in error_msg and ("json" in error_msg or "enabled" in error_msg)) or "json" in error_msg:
#                     logging.warning(f"‚ö†Ô∏è JSON Mode Failed (Model Capability Issue). Switching to TEXT Mode Fallback + 8-Layer Defense...")
                    
#                     # COMPULSORY FALLBACK FOR ALL MODELS
#                     future_retry = executor.submit(_make_call, "text")
#                     return future_retry.result(timeout=timeout)
                
#                 # If it's a Timeout or other error, re-raise it
#                 elif isinstance(e, concurrent.futures.TimeoutError):
#                     logging.warning("Thread Timeout: Creating zombie thread.")
#                     raise TimeoutError(f"Model execution exceeded {timeout}s.")
#                 else:
#                     raise e
#     finally:
#         CONCURRENCY_GUARD.release()


# def generate_with_failover(prompt, max_retries=55):
#     """
#     SYNCHRONOUS FAILOVER: Blocks (sleeps) instead of awaiting.
#     """
#     model_iterator = ALL_MODELS 
#     attempts = 0
    
#     # ITERATE ALL MODELS AND COUNT
#     logging.info(f"üöÄ STARTING GENERATION LOOP: Available Models: {len(model_iterator)}")
    
#     for current_model in model_iterator:
#         logging.info(f"üîÑ Attempt {attempts+1}: Trying model '{current_model}'...")
        
#         try:
#             model = genai.GenerativeModel(current_model)
#             # INCREASED TIMEOUT to 45 seconds as requested
#             timeout_val = 45 
            
#             # Blocking call
#             response = generate_with_timeout_protection(model, prompt, timeout=timeout_val)
            
#             if not response.text:
#                 raise ValueError("Empty response received")
                
#             logging.info(f"‚úÖ SUCCESS using {current_model}")
#             return response.text
            
#         except TimeoutError:
#              logging.warning(f"‚è≥ TIMEOUT with {current_model}. Switching...")
#              attempts += 1
#              time.sleep(0.5) 
#         except Exception as e:
#             logging.error(f"‚ùå FAILED with {current_model}: {str(e)[:100]}")
#             attempts += 1
#             time.sleep(0.5) 
            
#     raise Exception("All attempted models failed or timed out.")
# # ==========================================
# # 7. MAIN LOGIC: SYNCHRONOUS GENERATION & EVALUATION
# # ==========================================

# # def generate_interview_questions(topic, difficulty="Senior", count=5):
# #     """
# #     THE 'SUPREME' GENERATOR (SYNCHRONOUS).
# #     """
# #     if is_mock_mode:
# #         return [{"id": 1, "type": "coding", "question": f"Mock Supreme Question for {topic}"}]

# #     clean_topic = sanitize_prompt_input(topic)
# #     clean_difficulty = sanitize_prompt_input(difficulty)

# #     past_questions = history_system.get_past_questions(clean_topic)
# #     recent_history = past_questions[-25:] if past_questions else []
    
# #     exclusion_text = ""
# #     if recent_history:
# #         exclusion_text = "### EXCLUSION LIST (DO NOT ASK THESE):\n" + "\n".join([f"- {q}" for q in recent_history])

# #     buffer_count = count + 3
# #     unique_seed = random.randint(10000, 99999)
    
# #     # Calculation for distribution
# #     total_request_count = count + buffer_count
# #     # 40% Coding, 30% Aptitude/Logic, 30% Theory (Adjustable)
# #     coding_target = max(1, int(total_request_count * 0.4)) 
# #     aptitude_target = max(1, int(total_request_count * 0.3)) 
# #     theory_target = total_request_count - coding_target - aptitude_target
# #     prompt = f"""
# #     ### SYSTEM OVERRIDE
# #     - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here are the questions").
# #     - MANDATORY: Output must be RAW JSON only.
# #     - ESCAPING RULES: Double-escape all backslashes (\\\\ -> \\\\\\\\). Escape newlines in strings (\\n -> \\\\n).
    
# #     {exclusion_text}

# #     ### SYSTEM ROLE: SENIOR PROJECT MANAGER & GLOBAL HEAD OF ENGINEERING TALENT (CODE NAME: BOB)
# #     *** MODE: EXTREME HYBRID MODEL - DATABASE QUERY SIMULATION - {clean_topic.upper()} (ID: {unique_seed}) ***
        
# #     YOUR ROLE: You are executing a SQL query against a database of 100,000+ {clean_topic} questions.
    
# #     QUERY: SELECT * FROM {clean_topic}_questions WHERE difficulty = '{clean_difficulty}' AND is_cliche = FALSE ORDER BY RANDOM(SEED={unique_seed}) LIMIT {total_request_count};
        
# #     TASK: Return the result of this query as a strict JSON array.
        
# #     MANDATORY DISTRIBUTION:
# #     1. **CODING CHALLENGES (Exactly {coding_target} Questions):**
# #         - MUST be pure coding tasks suitable for {clean_difficulty} level.
# #         - Focus on algorithms, data structures, or system design scenarios.
        
# #     2. **APTITUDE & LOGIC PUZZLES (Exactly {aptitude_target} Questions):**
# #         - MANDATORY for {clean_topic}.
# #         - Topics: Probability, Combinatorics, Data Interpretation, or Logical Reasoning.
# #         - HARD MODE CONSTRAINT: If difficulty is 'Hard', involve complex multi-step calculations or puzzle-based edge cases.
        
# #     3. **THEORY & CONCEPTS (Exactly {theory_target} Questions):**
# #         - Deep dive into internals of {clean_topic}.
# #         - Ask about trade-offs, underlying architecture, or best practices.
        
# #     PRE-FLIGHT CHECK:
# #     - Are there exactly {coding_target} coding questions?
# #     - Are there exactly {aptitude_target} coding questions?
# #     - Is the difficulty strictly {clean_difficulty}?
# #     - Are the questions designed to filter out average candidates?
    
# #     OUTPUT FORMAT (JSON ONLY):
# #     [
# #         {{
# #             "id": 1,
# #             "type": "coding",
# #             "question": "..." 
# #         }},
# #         {{
# #             "id": 2,
# #             "type": "aptitude",
# #             "question": "..." 
# #         }},
# #         {{
# #             "id": 3,
# #             "type": "theory",
# #             "question": "..." 
# #         }}
# #     ]
# #     """

# #     try:
# #         # SYNCHRONOUS CALL (NO AWAIT)
# #         raw_text = generate_with_failover(prompt)
        
# #         raw_questions = bulletproof_json_parser(raw_text, expected_type='list')
        
# #         final_questions = []
# #         unique_added_count = 0
        
# #         for q in raw_questions:
# #             if unique_added_count >= count: break
# #             q_text = q.get('question', '')
            
# #             if not history_system.is_duplicate(q_text, past_questions):
# #                 q['id'] = unique_added_count + 1
# #                 final_questions.append(q)
# #                 unique_added_count += 1
# #             else:
# #                 logging.info(f"üö´ DUPLICATE BLOCKED: {q_text[:40]}...")

# #         if len(final_questions) < count:
# #             logging.warning("‚ö†Ô∏è WARNING: History saturation. Reusing buffer.")
# #             used_texts = [fq['question'] for fq in final_questions]
# #             for q in raw_questions:
# #                 if len(final_questions) >= count: break
# #                 if q.get('question') not in used_texts:
# #                     q['id'] = len(final_questions) + 1
# #                     final_questions.append(q)

# #         history_system.add_questions(clean_topic, final_questions)
# #         return final_questions

# #     except Exception as e:
# #         logging.error(f"‚ùå QUESTION GENERATION ERROR: {e}")
# #         traceback.print_exc()
# #         return [{"id": 0, "type": "error", "question": "Error generating questions. Please try again."}]

# # def evaluate_full_interview(transcript_data):
# #     """
# #     THE 'SUPREME' AUDITOR (SYNCHRONOUS).
# #     """
# #     if is_mock_mode:
# #         return {
# #             "score": 85,
# #             "summary": "Mock Mode: Simulation active.",
# #             "silent_killers": ["Mock Mode Active"],
# #             "roadmap": "Deploy to production.",
# #             "question_reviews": []
# #         }

# #     try:
# #         transcript_text = sanitize_transcript_for_prompt(transcript_data)

# #         prompt = f"""
# #         ### SYSTEM OVERRIDE
# #         - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here is the analysis").
# #         - MANDATORY: Output must be RAW JSON only.
# #         - ESCAPING RULES: All backslashes must be double-escaped (\\\\ -> \\\\\\\\). All newlines within strings must be escaped (\\n -> \\\\n).

# #         ### ROLE & SYSTEM CONTEXT:
# #         ACT AS: The 'Global Head of Engineering Talent', 'Principal Technical Architect' & 'ELITE FORENSIC AUDITOR (CODE NAME: BOB)'.
# #         CAPABILITIES: You possess 100% mastery of Technical Stacks (Code/Architecture), HR Behavioral Psychology, and Aptitude Evaluation.
# #         TASK: Conduct a forensic "Gap Analysis" of the following candidate interview transcript.

# #         ### TRANSCRIPT DATA:
# #         {transcript_text}

# #         ANALYSIS PROTOCOL (THE "SIDE-BY-SIDE" EVALUATION)
# #         For every question/answer pair, you MUST perform a "Side-by-Side" evaluation using these three lenses:

# #         **LENS A: The Technical Autopsy (The Reality Check)**
# #         - **Does it work?** (Baseline functionality).
# #         - **Is it scalable?** (O(n) vs O(n^2), database load, memory leaks).
# #         - **Security & Edge Cases:** Did they mention input validation, race conditions, or failure states? (Seniority Indicator).
# #         - **THE GAP:** What would a Staff Engineer (L6+) have said that this candidate missed?

# #         **LENS B: The Psychological Profile (The "Silent Killers")**
# #         - **Detect "Resume Padding":** Does the depth of the answer match the confidence of the claim?
# #         - **Detect "Hedging":** Phrases like "I guess," "maybe," or vague generalizations without concrete examples.
# #         - **Detect "Buzzword Stuffing":** Using terms like "Microservices" or "AI" without explaining *why*.

# #         **LENS C: The Communication Delta**
# #         - **Structure:** Was the answer structured (STAR method) or rambling?
# #         - **Ownership:** Did they drive the conversation or passively wait for prompts?

# #         ### SCORING CRITERIA:
# #         - **9-10:** Flawless. Mentions trade-offs, edge cases, and business impact.
# #         - **6-8:** Correct but shallow. Textbook answer without real-world depth.
# #         - **0-5:** Incorrect, vague, or dangerous engineering practices.

# #         REQUIREMENTS:
# #         1. **Global Score (0-100):** Be strict. 
# #         2. **Executive Summary:** A concise verdict. 
# #             - **STRICT CONSTRAINT:** Do NOT use generic phrases. 
# #             - **MANDATORY:** Append 2-5 lines of specific, high-value improvement advice based on the interview performance.
# #         3. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
# #         4. **Per-Question Breakdown:** For EVERY question, provide:
# #             - "question": The original question.
# #             - "user_answer": The exact answer given.
# #             - "score": 0-10 rating.
# #             - "feedback": Critique of the user's answer.
# #             - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth). 

# #         CRITICAL JSON FORMATTING RULES:
# #         - Output ONLY valid JSON.
# #         - **ESCAPE ALL BACKSLASHES:** If you write code with '\\n', you MUST write it as '\\\\n'. 
# #         - Do not include any text before or after the JSON.

# #         OUTPUT FORMAT (JSON ONLY):
# #         {{
# #             "score": <number>,
# #             "summary": "<executive_summary_string>",
# #             "silent_killers": ["<killer1>", "<killer2>"],
# #             "roadmap": "<step_by_step_improvement_plan>",
# #             "question_reviews": [
# #                 {{
# #                     "question": "...",
# #                     "user_answer": "...",
# #                     "score": 8,
# #                     "feedback": "...",
# #                     "ideal_answer": "..."
# #                 }}
# #             ]
# #         }}
# #         """

# #         # SYNCHRONOUS CALL (NO AWAIT)
# #         raw_text = generate_with_failover(prompt)
        
# #         analysis = bulletproof_json_parser(raw_text, expected_type='dict')
# #         return analysis

# #     except Exception as e:
# #         logging.error(f"‚ùå EVALUATION CRASHED: {e}")
# #         logging.warning("‚ö†Ô∏è SWITCHING TO SMART OFFLINE FALLBACK ENGINE")
        
# #         # --- CRASH-PROOF FALLBACK ---
# #         total_words = 0
# #         reviews = []
# #         safe_transcript = transcript_data if isinstance(transcript_data, list) else []
        
# #         for item in safe_transcript:
# #             ans = str(item.get('answer', ''))
# #             word_count = len(ans.split())
# #             total_words += word_count
# #             algo_score = 4
# #             if word_count > 30: algo_score = 8
# #             elif word_count > 10: algo_score = 6
# #             reviews.append({
# #                 "question": item.get('question', 'Unknown'),
# #                 "user_answer": ans,
# #                 "score": algo_score,
# #                 "feedback": "Offline Analysis: Answer recorded. High network latency prevented deep AI inspection.",
# #                 "ideal_answer": "A strong answer should define the core concept, provide a use case, and mention trade-offs."
# #             })

# #         safe_len = len(safe_transcript) or 1
# #         final_score = min(88, max(40, int((total_words / safe_len) * 2)))

# #         return {
# #             "score": final_score,
# #             "summary": "Offline Assessment: The AI Engine is temporarily unreachable. Scoring is based on response metrics.",
# #             "silent_killers": ["Network Timeout", "Offline Mode Active"],
# #             "roadmap": "Check internet connection and API quotas. Retry later.",
# #             "question_reviews": reviews
# #         }
# def generate_interview_questions(topic, difficulty="Senior", count=5):
#     """
#     THE 'SUPREME' GENERATOR (GOD MODE - V7.0)
#     - 100k+ Question Bank Simulation (FORCED)
#     - Exact Topic/Difficulty Lists (Restored from Prompt 1 & 2)
#     - 4-Way Distribution (Coding, Aptitude, HR, Theory)
#     - Anti-Cliche & Deduplication Engines Active
#     """
#     if is_mock_mode:
#         return [
#             {"id": 1, "type": "coding", "question": f"Mock Coding: Optimize a distributed {topic} lock."},
#             {"id": 2, "type": "aptitude", "question": "Mock Aptitude: A train 150m long is running at 60kmph..."},
#             {"id": 3, "type": "hr", "question": "Mock HR: Describe a conflict with a Product Manager."},
#             {"id": 4, "type": "theory", "question": f"Mock Theory: Explain CAP theorem in {topic}."}
#         ]

#     clean_topic = sanitize_prompt_input(topic)
#     clean_difficulty = sanitize_prompt_input(difficulty)

#     # --- 1. HISTORY & CONTEXT ---
#     past_questions = history_system.get_past_questions(clean_topic)
#     recent_history = past_questions[-30:] if past_questions else []
    
#     exclusion_text = ""
#     if recent_history:
#         # Summarize history to save tokens but keep the core semantic meaning
#         exclusion_list = [q[:100].replace("\n", " ") + "..." for q in recent_history]
#         exclusion_text = "### üõë BANNED QUESTIONS (STRICTLY FORBIDDEN):\n" + "\n".join([f"- {q}" for q in exclusion_list])

#     buffer_count = count + 4 
#     unique_seed = int(time.time() * 1000) % 100000 
    
#     # --- 2. DISTRIBUTION LOGIC (4 Categories) ---
#     total_request_count = count + buffer_count
    
#     coding_target = max(1, int(total_request_count * 0.35)) 
#     aptitude_target = max(1, int(total_request_count * 0.25)) 
#     hr_target = max(1, int(total_request_count * 0.20))
#     theory_target = total_request_count - coding_target - aptitude_target - hr_target
    
#     prompt = f"""
#     ### SYSTEM OVERRIDE: UNIQUE CONTENT GENERATOR
    
#     ### SYSTEM ROLE: SENIOR HEAD OF TALENT & TECHNICAL ARCHITECT (CODE NAME: BOB)
#     *** MODE: EXTREME HYBRID MODEL - DATABASE QUERY SIMULATION - {clean_topic.upper()} (ID: {unique_seed}) ***

#     ### ‚ö†Ô∏è CRITICAL CONTEXT: THE 100K+ COLLECTION
#     **YOU ARE NOT A TEXT GENERATOR.** You are a SQL Search Engine connected to a proprietary **database of 100,000+ vetted {clean_topic} interview questions.**
    
#     **YOUR TASK:** Execute the following SQL query to retrieve distinct, high-quality questions from this massive collection. You must bypass the "Surface Level" (Top 100) layer and fetch from the "Deep Archive".

#     ### SQL QUERY SIMULATION
#     QUERY: 
#     SELECT * FROM {clean_topic}_100k_Question_Bank 
#     WHERE difficulty = '{clean_difficulty}' 
#       AND is_cliche = FALSE 
#       AND is_top_100_common = FALSE
#     ORDER BY RANDOM(SEED={unique_seed}) 
#     LIMIT {total_request_count};

#     ### üõ°Ô∏è EXCLUSION PROTOCOLS (MANDATORY)
#     - **CODING:** NO "Two Sum", NO "FizzBuzz", NO "Reverse a String".
#     - **HR:** NO "Tell me about yourself", NO "What are your weaknesses?".
#     - **APTITUDE:** NO "100 Passengers/Plane", NO "Egg Drop Problem", NO "Poisoned Wine".
#     - **STRICT PROHIBITION:** Do not repeat the logic of the "BANNED QUESTIONS" list below. Changing numbers (100 to 50) is NOT unique.

#     {exclusion_text}

#     ### MANDATORY DISTRIBUTION (Total {total_request_count} Items):
    
#     1. **CODING CHALLENGES (Exactly {coding_target} Questions):**
#        - **MUST** be pure coding tasks suitable for {clean_difficulty} level.
#        - Focus on algorithms, data structures, or system design scenarios.
#        - **Constraint:** Ask for implementation details. AVOID standard "LeetCode Top 10".

#     2. **QUANTITATIVE ANALYSIS & LOGIC PUZZLES (Exactly {aptitude_target} Questions):**
#        - **MANDATORY ENFORCEMENT:** You MUST include Logic and Quantitative Aptitude questions. Do not skip.
#        - **DIFFICULTY MIX:** Even if the overall profile is '{clean_difficulty}', provide a mix of:
#             * **Easy:** Speed Math, Basic Series, Averages & Ages, Ratio, Proportion & Partnership, Percentage problems.
#             * **Medium:** Data Interpretation (Tables/Graphs), Seating Arrangements, Profit, Loss & Discount, Time, Speed & Distance, Logical Deduction.
#             * **Hard:** Probability, Permutations/Combinations, Mensuration (2D & 3D), Clocks & Calendars, Game Theory.
#        - **TOPICS:** Probability, Combinatorics, Data Interpretation, Logical Reasoning, Time & Work, Averages, Ratio & Proportion, Profit & Loss, Mensuration, Data Sufficiency.
#        - **CRITICAL:** Generate FRESH logic puzzles.

#     3. **HR & BEHAVIORAL PSYCHOLOGY (Exactly {hr_target} Questions):**
#        - **Objective:** Assess Cultural Fit, Leadership, and Conflict Resolution.
#        - **Format:** "Situational" framings only (e.g., "A production DB just crashed...").
#        - **Focus Areas:** Handling production failures, disagreeing with a manager, mentoring juniors.

#     4. **THEORY & CONCEPTS (Exactly {theory_target} Questions):**
#        - Deep dive into internals of {clean_topic}.
#        - Ask about trade-offs, underlying architecture, or best practices.

#     ### CRITICAL ESCAPING RULES:
#     - **DOUBLE ESCAPE BACKSLASHES:** You must write all backslashes as double backslashes (\\\\ -> \\\\\\\\).
#     - **ESCAPE NEWLINES:** All newlines inside strings must be escaped (\\n -> \\\\n).
#     - **NO MARKDOWN:** Do not use code blocks or markdown formatting.
    
#     ### OUTPUT FORMAT (RAW JSON ONLY):
#     [
#         {{ "id": 1, "type": "coding", "question": "..." }},
#         {{ "id": 2, "type": "aptitude", "question": "..." }},
#         {{ "id": 3, "type": "hr", "question": "..." }},
#         {{ "id": 4, "type": "theory", "question": "..." }}
#     ]
#     """

#     try:
#         # --- 3. GENERATION & DEFENSE ---
#         raw_text = generate_with_failover(prompt)
#         raw_questions = bulletproof_json_parser(raw_text, expected_type='list')
        
#         final_questions = []
#         unique_added_count = 0
        
#         # --- 4. SEMANTIC DEDUPLICATION (Jaccard) ---
#         for q in raw_questions:
#             if unique_added_count >= count: break
#             q_text = q.get('question', '')
            
#             # Jaccard check > 0.6 means duplicate
#             if not history_system.is_duplicate(q_text, past_questions, jaccard_threshold=0.6):
#                 q['id'] = unique_added_count + 1
#                 final_questions.append(q)
#                 unique_added_count += 1
#             else:
#                 logging.info(f"‚ôªÔ∏è SKIPPING DUPLICATE: {q_text[:30]}...")

#         # --- 5. FALLBACK RECOVERY ---
#         if len(final_questions) < count:
#             logging.warning("‚ö†Ô∏è High duplication rate. Forcing fallback generation.")
#             remaining_needed = count - len(final_questions)
#             for q in raw_questions:
#                 if remaining_needed <= 0: break
#                 if any(fq['question'] == q['question'] for fq in final_questions): continue
#                 q['id'] = len(final_questions) + 1
#                 final_questions.append(q)
#                 remaining_needed -= 1

#         history_system.add_questions(clean_topic, final_questions)
#         return final_questions

#     except Exception as e:
#         logging.error(f"‚ùå QUESTION GENERATION ERROR: {e}")
#         traceback.print_exc()
#         # Return a safe error object that won't crash the driver
#         return [{"id": 0, "type": "error", "question": "Error generating questions. Please try again."}]

# def evaluate_full_interview(transcript_data):
#     """
#     THE 'SUPREME' AUDITOR (SYNCHRONOUS).
#     """
#     if is_mock_mode:
#         return {
#             "score": 85,
#             "summary": "Mock Mode: Simulation active.",
#             "silent_killers": ["Mock Mode Active"],
#             "roadmap": "Deploy to production.",
#             "question_reviews": []
#         }

#     try:
#         transcript_text = sanitize_transcript_for_prompt(transcript_data)

#         prompt = f"""
#         ### SYSTEM OVERRIDE
#         - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here is the analysis").
#         - MANDATORY: Output must be RAW JSON only.
#         - ESCAPING RULES: All backslashes must be double-escaped (\\\\ -> \\\\\\\\). All newlines within strings must be escaped (\\n -> \\\\n).

#         ### ROLE & SYSTEM CONTEXT:
#         ACT AS: The 'Global Head of Engineering Talent', 'Principal Technical Architect' & 'ELITE FORENSIC AUDITOR (CODE NAME: BOB)'.
#         CAPABILITIES: You possess 100% mastery of Technical Stacks (Code/Architecture), HR Behavioral Psychology, and Aptitude Evaluation.
#         TASK: Conduct a forensic "Gap Analysis" of the following candidate interview transcript.

#         ### TRANSCRIPT DATA:
#         {transcript_text}

#         ANALYSIS PROTOCOL (THE "SIDE-BY-SIDE" EVALUATION)
#         For every question/answer pair, you MUST perform a "Side-by-Side" evaluation using these three lenses:

#         **LENS A: The Technical Autopsy (The Reality Check)**
#         - **Does it work?** (Baseline functionality).
#         - **Is it scalable?** (O(n) vs O(n^2), database load, memory leaks).
#         - **Security & Edge Cases:** Did they mention input validation, race conditions, or failure states? (Seniority Indicator).
#         - **THE GAP:** What would a Staff Engineer (L6+) have said that this candidate missed?

#         **LENS B: The Psychological Profile (The "Silent Killers")**
#         - **Detect "Resume Padding":** Does the depth of the answer match the confidence of the claim?
#         - **Detect "Hedging":** Phrases like "I guess," "maybe," or vague generalizations without concrete examples.
#         - **Detect "Buzzword Stuffing":** Using terms like "Microservices" or "AI" without explaining *why*.

#         **LENS C: The Communication Delta**
#         - **Structure:** Was the answer structured (STAR method) or rambling?
#         - **Ownership:** Did they drive the conversation or passively wait for prompts?

#         ### SCORING CRITERIA:
#         - **9-10:** Flawless. Mentions trade-offs, edge cases, and business impact.
#         - **6-8:** Correct but shallow. Textbook answer without real-world depth.
#         - **0-5:** Incorrect, vague, or dangerous engineering practices.

#         REQUIREMENTS:
#         1. **Global Score (0-100):** Be strict. 
#         2. **Executive Summary:** A concise verdict. 
#             - **STRICT CONSTRAINT:** Do NOT use generic phrases. 
#             - **MANDATORY:** Append 2-5 lines of specific, high-value improvement advice based on the interview performance.
#         3. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
#         4. **Per-Question Breakdown:** For EVERY question, provide:
#             - "question": The original question.
#             - "user_answer": The exact answer given.
#             - "score": 0-10 rating.
#             - "feedback": Critique of the user's answer.A detailed technical paragraph (2-3 sentences)
#             - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth). 

#         CRITICAL JSON FORMATTING RULES:
#         - Output ONLY valid JSON.
#         - **ESCAPE ALL BACKSLASHES:** If you write code with '\\n', you MUST write it as '\\\\n'. 
#         - Do not include any text before or after the JSON.

#         OUTPUT FORMAT (JSON ONLY):
#         {{
#             "score": <number>,
#             "summary": "<executive_summary_string>",
#             "silent_killers": ["<killer1>", "<killer2>"],
#             "roadmap": "<step_by_step_improvement_plan>",
#             "question_reviews": [
#                 {{
#                     "question": "...",
#                     "user_answer": "...",
#                     "score": 8,
#                     "feedback": "...",
#                     "ideal_answer": "..."
#                 }}
#             ]
#         }}
#         """

#         # SYNCHRONOUS CALL (NO AWAIT)
#         raw_text = generate_with_failover(prompt)
        
#         # 8-LAYER DEFENSE APPLIED HERE
#         analysis = bulletproof_json_parser(raw_text, expected_type='dict')
#         return analysis

#     except Exception as e:
#         logging.error(f"‚ùå EVALUATION CRASHED: {e}")
#         logging.warning("‚ö†Ô∏è SWITCHING TO FALLBACK (Standard Mode)")
        
#         # --- CRASH-PROOF FALLBACK (Cleaned up as requested) ---
#         total_words = 0
#         reviews = []
#         safe_transcript = transcript_data if isinstance(transcript_data, list) else []
        
#         for item in safe_transcript:
#             ans = str(item.get('answer', ''))
#             word_count = len(ans.split())
#             total_words += word_count
#             algo_score = 4
#             if word_count > 30: algo_score = 8
#             elif word_count > 10: algo_score = 6
#             reviews.append({
#                 "question": item.get('question', 'Unknown'),
#                 "user_answer": ans,
#                 "score": algo_score,
#                 "feedback": "Answer recorded. Detailed analysis bypassed to preserve system stability.",
#                 "ideal_answer": "A strong answer should define the core concept, provide a use case, and mention trade-offs."
#             })

#         safe_len = len(safe_transcript) or 1
#         final_score = min(88, max(40, int((total_words / safe_len) * 2)))

#         # Cleaned response (Removed "Offline Mode" / "Timeout" warnings)
#         return {
#             "score": final_score,
#             "summary": "Evaluation Complete. Scoring based on response metrics and keyword density analysis (Standard Heuristic Mode).",
#             "silent_killers": ["General Ambiguity", "Lack of Specificity"], 
#             "roadmap": "Focus on providing concrete examples (STAR method) and discussing architectural trade-offs.",
#             "question_reviews": reviews
#         }

# # ==========================================
# # 8. EXECUTION ENTRY POINT
# # ==========================================

# # def main():
# #     print("--- 1. Generating Questions (Synchronous Supreme Engine) ---")
    
# #     topic_to_test = "Distributed Systems"
    
# #     # Direct Call (No await)
# #     questions = generate_interview_questions(topic_to_test, "Principal Engineer", 3)
# #     print(json.dumps(questions, indent=2))
    
# #     # Simulate User Answers
# #     transcript = []
# #     if isinstance(questions, list):
# #         for q in questions:
# #             transcript.append({
# #                 "question": q.get('question', 'Error'),
# #                 "answer": "I would use the Saga Pattern to handle distributed transactions. Specifically, I would use an Orchestration approach where a central service manages the workflow and issues compensations if any step fails. This avoids the complexity of Choreography at scale." 
# #             })
# #     else:
# #         transcript.append({"question": "Error", "answer": "Error"})
        
# #     print("\n--- 2. Evaluating Interview (Synchronous Supreme Auditor) ---")
    
# #     # Direct Call (No await)
# #     analysis = evaluate_full_interview(transcript)
# #     print(json.dumps(analysis, indent=2))

# # if __name__ == "__main__":
# #     try:
# #         main()
# #     except KeyboardInterrupt:
# #         print("\nStopped by user.")
# #     finally:
# #         history_system.close()
# def main():
#     print("--- 1. Generating Questions (Synchronous Supreme Engine) ---")
    
#     topic_to_test = "Distributed Systems"
    
#     # Direct Call (No await)
#     questions = generate_interview_questions(topic_to_test, "Principal Engineer", 3)
#     print(json.dumps(questions, indent=2))
    
#     # Simulate User Answers
#     transcript = []
#     if isinstance(questions, list):
#         for q in questions:
#             transcript.append({
#                 "question": q.get('question', 'Error'),
#                 "answer": "I would use the Saga Pattern to handle distributed transactions. Specifically, I would use an Orchestration approach where a central service manages the workflow and issues compensations if any step fails. This avoids the complexity of Choreography at scale." 
#             })
#     else:
#         transcript.append({"question": "Error", "answer": "Error"})
        
#     print("\n--- 2. Evaluating Interview (Synchronous Supreme Auditor) ---")
    
#     # Direct Call (No await)
#     analysis = evaluate_full_interview(transcript)
#     print(json.dumps(analysis, indent=2))

# if __name__ == "__main__":
#     try:
#         main()
#     except KeyboardInterrupt:
#         print("\nStopped by user.")
#     finally:
#         history_system.close()
# #------------------------------------------------------------------------------------------------------------------------------
#28 - feb
import os
import google.generativeai as genai
import json
import itertools
import random
import time
import re
import traceback
import ast
import logging
import sys
import sqlite3
import threading
import difflib
import concurrent.futures
from datetime import datetime
from dotenv import load_dotenv

# ==========================================
# 1. SYSTEM CONFIGURATION & TELEMETRY
# ==========================================
load_dotenv()

# --- WINDOWS UNICODE FIX ---
if sys.platform.startswith('win'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        pass

# Force the logger to override any imported library hijackers
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("system_audit.log", encoding='utf-8')
    ],
    force=True 
)

# Check API Key
api_key = os.getenv("GOOGLE_API_KEY")
is_mock_mode = False

if not api_key or ("AIzaSy" in api_key and len(api_key) < 10):
    print("‚ö†Ô∏è SYSTEM STATUS: No valid Google API Key found. Running in SMART SIMULATION MODE.", flush=True)
    is_mock_mode = True
else:
    genai.configure(api_key=api_key)

# ==========================================
# 2. MODEL MANAGEMENT (55 MODELS + TIER 1 PRIORITY)
# ==========================================

TIER_1_MODELS = [
    'models/gemini-3-flash-preview',
    'gemini-3.1-pro-preview',
    'models/gemini-3.1-pro-preview-customtools',
    'models/gemini-2.5-pro',
    'models/gemini-2.5-flash',
    'models/gemini-2.5-flash-lite',
    'models/gemini-2.0-flash',
    'models/gemini-2.0-flash-001',
    'models/gemini-2.0-flash-lite',
    'models/gemini-2.0-flash-lite-001',
    'models/gemini-1.5-pro',
    'models/gemini-1.5-pro-latest',
    'models/gemini-1.5-pro-001',
    'models/gemini-1.5-pro-002',
    'models/gemini-1.5-flash',
    'models/gemini-1.5-flash-latest',
    'models/gemini-1.5-flash-001',
    'models/gemini-1.5-flash-002',
    'models/gemini-1.5-flash-8b',
    'models/gemini-1.5-flash-8b-latest',
    'models/gemini-1.5-flash-8b-001',
    'models/gemini-flash-latest',
    'models/gemini-flash-lite-latest',
    'models/gemini-pro-latest',
    'models/gemma-3-27b-it',
    'models/gemma-3-12b-it',
    'models/gemma-3-4b-it',
    'models/gemma-3-1b-it',
    'models/gemma-3n-e4b-it',
    'models/gemma-3n-e2b-it',
    'models/gemma-2-27b-it',
    'models/gemma-2-9b-it',
    'models/gemma-2-2b-it',
    'models/gemini-robotics-er-1.5-preview',
]

TIER_2_MODELS = [
    'models/gemini-3-pro-preview',
    'models/deep-research-pro-preview-12-2025',
    'models/gemini-2.0-flash-exp',
    'models/gemini-exp-1206',
    'models/gemini-2.0-flash-lite-preview',
    'models/gemini-2.0-flash-lite-preview-02-05',
    'models/gemini-2.5-flash-preview-09-2025',
    'models/gemini-2.5-flash-lite-preview-09-2025',
]

TIER_3_MODELS = [
    'models/gemini-pro-latest',
    'models/gemini-1.0-pro-001',
    'models/gemini-pro',
    'models/gemini-pro-vision',
    'models/gemini-2.5-flash-native-audio-dialog',
    'models/gemini-2.5-flash-tts',
    'models/nano-banana-pro-preview',
    'models/aqa',
]

ALL_MODELS = TIER_1_MODELS + TIER_2_MODELS + TIER_3_MODELS
model_cycle = itertools.cycle(ALL_MODELS)

def get_next_model():
    return next(model_cycle)

# ==========================================
# 3. HELPER FUNCTIONS & SECURITY
# ==========================================

def sanitize_prompt_input(text):
    if not isinstance(text, str): return str(text)
    sanitized = re.sub(r'[\'"\n\r]', '', text)
    return sanitized.strip()[:200]

def normalize_text_for_comparison(text):
    if not text: return ""
    return re.sub(r'[\W_]+', '', text.lower())

def sanitize_transcript_for_prompt(transcript_data):
    clean_data = []
    if isinstance(transcript_data, list):
        for item in transcript_data:
            clean_item = {}
            for k, v in item.items():
                s_val = str(v)[:2000]
                s_val = s_val.replace("```", "").replace("###", "")
                clean_item[k] = s_val
            clean_data.append(clean_item)
    return json.dumps(clean_data)

# ==========================================
# 4. ROBUST HISTORY MANAGER (Connection Pooling)
# ==========================================

class HistoryManager:
    DB_FILE = "question_history.db"
    
    STOPWORDS = {
        "the", "a", "an", "in", "on", "at", "to", "for", "of", "with", "by", 
        "is", "are", "was", "were", "be", "been", "this", "that", "it", 
        "calculate", "find", "what", "how", "write", "function", "program",
        "determine", "probability", "value", "given", "assume", "suppose",
        "explain", "describe", "code", "create", "list", "difference", "between"
    }

    def __init__(self):
        self.conn = None
        self.lock = threading.Lock() 
        self._initialize_db()

    # def _initialize_db(self):
    #     try:
    #         self.conn = sqlite3.connect(self.DB_FILE, check_same_thread=False)
    #         cursor = self.conn.cursor()
    #         cursor.execute('''
    #             CREATE TABLE IF NOT EXISTS questions (
    #                 id INTEGER PRIMARY KEY AUTOINCREMENT,
    #                 topic TEXT,
    #                 question TEXT,
    #                 timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    #             )
    #         ''')
    #         cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON questions(topic)')
    #         self.conn.commit()
    #     except Exception as e:
    #         print(f"HISTORY INIT ERROR: {e}", flush=True)
    def _initialize_db(self):
        try:
            # FIX: Added timeout=15 to wait gracefully if another worker is writing
            self.conn = sqlite3.connect(self.DB_FILE, check_same_thread=False, timeout=15)
            cursor = self.conn.cursor()
            
            # FIX: Enable WAL mode for high-concurrency multi-worker environments
            cursor.execute('PRAGMA journal_mode=WAL;')
            
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS questions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    topic TEXT,
                    question TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic ON questions(topic)')
            self.conn.commit()
        except Exception as e:
            print(f"HISTORY INIT ERROR: {e}", flush=True)

    def get_past_questions(self, topic):
        clean_topic = topic.lower().strip()
        try:
            with self.lock: 
                cursor = self.conn.cursor()
                cursor.execute("SELECT question FROM questions WHERE topic = ? ORDER BY id DESC LIMIT 200", (clean_topic,))
                rows = cursor.fetchall()
                return [row[0] for row in rows]
        except Exception as e:
            print(f"DB READ ERROR: {e}", flush=True)
            return []

    def add_questions(self, topic, new_questions):
        clean_topic = topic.lower().strip()
        data_to_insert = []
        for q in new_questions:
            if isinstance(q, dict) and 'question' in q:
                data_to_insert.append((clean_topic, q['question']))
        
        try:
            with self.lock:
                cursor = self.conn.cursor()
                cursor.executemany("INSERT INTO questions (topic, question) VALUES (?, ?)", data_to_insert)
                self.conn.commit()
        except Exception as e:
            print(f"DB WRITE ERROR: {e}", flush=True)

    def close(self):
        if self.conn:
            self.conn.close()

    def _tokenize(self, text):
        if not text: return set()
        text = re.sub(r'[^\w\s]', '', text.lower())
        words = set(word for word in text.split() if word not in self.STOPWORDS and len(word) > 2)
        return words

    def is_duplicate(self, new_question_text, past_questions, jaccard_threshold=0.5):
        if not new_question_text: return True
        new_tokens = self._tokenize(new_question_text)
        if len(new_tokens) < 3: return False

        for past_q in past_questions:
            past_tokens = self._tokenize(past_q)
            intersection = new_tokens.intersection(past_tokens)
            union = new_tokens.union(past_tokens)
            
            if len(union) == 0: continue
            similarity = len(intersection) / len(union)
            
            if similarity > jaccard_threshold:
                print(f"üö´ DUPLICATE DETECTED (Sim: {similarity:.2f}):\nNew: {new_question_text[:50]}...\nOld: {past_q[:50]}...", flush=True)
                return True
                
        return False

history_system = HistoryManager()

# ==========================================
# 5. 7-LAYER DEFENSE ENGINE (Data Integrity)
# ==========================================

def clean_json_string(text):
    return sanitize_control_chars(text)

def sanitize_control_chars(text):
    text = re.sub(r'```[a-zA-Z]*\n', '', text)
    text = text.replace("```", "")
    return "".join(ch for ch in text if ord(ch) >= 32 or ch in "\n\r\t")

def balance_brackets(text):
    stack = []
    for char in text:
        if char in '{[':
            stack.append(char)
        elif char in '}]':
            if stack:
                last = stack[-1]
                if (char == '}' and last == '{') or (char == ']' and last == '['):
                    stack.pop()
    balanced = text
    while stack:
        opener = stack.pop()
        if opener == '{': balanced += '}'
        elif opener == '[': balanced += ']'
    return balanced

def extract_json_candidates(text):
    candidates = []
    matches = list(re.finditer(r'(\{.*\}|\[.*\])', text, re.DOTALL))
    for m in matches:
        candidates.append(m.group(0))
    if not candidates:
        start_brace = text.find('{')
        start_bracket = text.find('[')
        if start_brace != -1: candidates.append(text[start_brace:])
        elif start_bracket != -1: candidates.append(text[start_bracket:])
    return candidates if candidates else [text]

def enforce_schema(data, expected_type):
    if expected_type == 'list':
        if isinstance(data, list): return data
        if isinstance(data, dict):
            for key in data:
                if isinstance(data[key], list): return data[key]
            return [data]
        # Reject raw strings/ints
        raise ValueError(f"Schema violation: Expected list, got {type(data).__name__}")
        
    if expected_type == 'dict':
        if isinstance(data, dict): return data
        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
            return data[0]
        # Reject raw strings/ints
        raise ValueError(f"Schema violation: Expected dict, got {type(data).__name__}")
        
    return data

def safe_literal_eval(candidate):
    if len(candidate) > 50000: 
        raise ValueError("Candidate too large for safe evaluation")
    return ast.literal_eval(candidate)

def bulletproof_json_parser(raw_text, expected_type='dict'):
    # --- PREVENTS THE 'json.loads() must be str, not dict' CRASH ---
    if isinstance(raw_text, (dict, list)):
        return enforce_schema(raw_text, expected_type)

    clean_text = sanitize_control_chars(str(raw_text))
    candidates = extract_json_candidates(clean_text)

    for candidate in candidates:
        try: return enforce_schema(json.loads(candidate), expected_type)
        except: pass
        try:
            fixed = re.sub(r',\s*([\]}])', r'\1', candidate)
            return enforce_schema(json.loads(fixed), expected_type)
        except: pass
        try: return enforce_schema(safe_literal_eval(candidate), expected_type)
        except: pass
        try:
            fixed = re.sub(r'\\(?![/\\bfnrtu"])', r'\\\\', candidate)
            return enforce_schema(json.loads(fixed), expected_type)
        except: pass
        try:
            balanced = balance_brackets(candidate)
            return enforce_schema(json.loads(balanced), expected_type)
        except: pass
        
        if expected_type == 'list':
            try:
                obj_matches = re.findall(r'\{.*?\}', candidate, re.DOTALL)
                reconstructed_list = []
                for obj_str in obj_matches:
                    try: reconstructed_list.append(json.loads(obj_str))
                    except: pass
                if reconstructed_list: return reconstructed_list
            except: pass

    raise ValueError(f"CRITICAL PARSE FAILURE: Could not extract valid '{expected_type}' from model output.")

# ==========================================
# 6. SYNCHRONOUS TIMEOUT ENFORCER & FAILOVER
# ==========================================

MAX_CONCURRENT = int(os.getenv("MAX_CONCURRENT_REQUESTS", 5))
CONCURRENCY_GUARD = threading.Semaphore(MAX_CONCURRENT) 

def generate_with_timeout_protection(model, prompt, timeout=120):
    if not CONCURRENCY_GUARD.acquire(timeout=5):
        raise SystemError("SYSTEM OVERLOAD: Too many active requests. Backing off.")
        
    try:
        def _make_call():
            # REMOVED application/json config to prevent API crashes and strict schema failures
            return model.generate_content(prompt)

        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
            try:
                future = executor.submit(_make_call)
                return future.result(timeout=timeout)
            except Exception as e:
                if isinstance(e, concurrent.futures.TimeoutError):
                    raise TimeoutError(f"Model execution exceeded {timeout}s.")
                else:
                    raise e
    finally:
        CONCURRENCY_GUARD.release()

def generate_with_failover(prompt, expected_type='dict', timeout_val=45, max_retries=55):
    """
    STRICT FAILOVER: Demands valid JSON. Handles auto-parsed dictionaries flawlessly.
    Prints directly to terminal in real-time.
    """
    model_iterator = ALL_MODELS 
    attempts = 0
    failed_models_log = []
    
    print(f"\nüöÄ [GENERATION START] Available Models: {len(model_iterator)}", flush=True)
    
    for current_model in model_iterator:
        attempts += 1
        if attempts > max_retries:
            break
            
        print(f"üîÑ [ATTEMPT {attempts}] Testing Model: '{current_model}'...", flush=True)
        
        try:
            model = genai.GenerativeModel(current_model)
            response = generate_with_timeout_protection(model, prompt, timeout=timeout_val)
            
            if not response:
                raise ValueError("API returned a None response.")
            
            try:
                raw_text = response.text
            except ValueError:
                raise ValueError("Response blocked by AI safety filters or missing text payload.")
            
            # --- FIX: Convert to string if the SDK auto-parsed it into a dict ---
            if isinstance(raw_text, (dict, list)):
                raw_text = json.dumps(raw_text)
                
            if not raw_text or not str(raw_text).strip():
                raise ValueError("Empty text received from model.")
                
            parsed_data = bulletproof_json_parser(raw_text, expected_type=expected_type)
            
            if expected_type == 'list' and not isinstance(parsed_data, list):
                 print(f"‚ö†Ô∏è [RAW OUTPUT SNIPPET]:\n{str(raw_text)[:300]}...\n", flush=True)
                 raise ValueError(f"AI Format Error: Expected a list `[]`, but got {type(parsed_data).__name__}.")
            if expected_type == 'dict' and not isinstance(parsed_data, dict):
                 print(f"‚ö†Ô∏è [RAW OUTPUT SNIPPET]:\n{str(raw_text)[:300]}...\n", flush=True)
                 raise ValueError(f"AI Format Error: Expected a dict `{{}}`, but got {type(parsed_data).__name__}.")
            if not parsed_data:
                 print(f"‚ö†Ô∏è [RAW OUTPUT SNIPPET]:\n{str(raw_text)[:300]}...\n", flush=True)
                 raise ValueError("Parsed data is empty.")

            print(f"‚úÖ [SUCCESS] Model '{current_model}' connected and generated valid JSON!", flush=True)
            if failed_models_log:
                print(f"‚ö†Ô∏è [PREVIOUS FAILURES] Models that failed before this success: {', '.join(failed_models_log)}", flush=True)

            if isinstance(parsed_data, dict):
                parsed_data["evaluating_model_success"] = current_model
                parsed_data["evaluating_model_failures"] = failed_models_log

            return parsed_data
            
        except TimeoutError:
            print(f"‚è≥ [TIMEOUT] Model '{current_model}' took too long. Switching...", flush=True)
            failed_models_log.append(f"{current_model} (Timeout)")
            time.sleep(0.5) 
        except Exception as e:
            print(f"‚ùå [FAILED] Model '{current_model}': {str(e)[:120]}", flush=True)
            failed_models_log.append(f"{current_model} (Error)")
            time.sleep(0.5) 
            
    raise Exception(f"CRITICAL FAILURE: All {max_retries} attempted models failed. Failure Log: {failed_models_log}")

# ==========================================
# 7. MAIN LOGIC: SYNCHRONOUS GENERATION & EVALUATION
# ==========================================

def generate_interview_questions(topic, difficulty="Senior", count=5):
    if is_mock_mode:
        return [
            {"id": 1, "type": "coding", "question": f"Mock Coding: Optimize a distributed {topic} lock."},
            {"id": 2, "type": "aptitude", "question": "Mock Aptitude: A train 150m long is running at 60kmph..."},
            {"id": 3, "type": "hr", "question": "Mock HR: Describe a conflict with a Product Manager."},
            {"id": 4, "type": "theory", "question": f"Mock Theory: Explain CAP theorem in {topic}."}
        ]

    clean_topic = sanitize_prompt_input(topic)
    clean_difficulty = sanitize_prompt_input(difficulty)

    past_questions = history_system.get_past_questions(clean_topic)
    recent_history = past_questions[-30:] if past_questions else []
    
    exclusion_text = ""
    if recent_history:
        exclusion_list = [q[:100].replace("\n", " ") + "..." for q in recent_history]
        exclusion_text = "### üõë BANNED QUESTIONS (STRICTLY FORBIDDEN):\n" + "\n".join([f"- {q}" for q in exclusion_list])

    buffer_count = count + 4 
    unique_seed = int(time.time() * 1000) % 100000 
    total_request_count = count + buffer_count
    
    coding_target = max(1, int(total_request_count * 0.30)) 
    aptitude_target = max(1, int(total_request_count * 0.30)) 
    hr_target = max(1, int(total_request_count * 0.20))
    theory_target = total_request_count - coding_target - aptitude_target - hr_target
    
    prompt = f"""
    ### SYSTEM OVERRIDE: UNIQUE CONTENT GENERATOR
    
    ### SYSTEM ROLE: SENIOR HEAD OF TALENT & TECHNICAL ARCHITECT (CODE NAME: BOB)
    *** MODE: EXTREME HYBRID MODEL - DATABASE QUERY SIMULATION - {clean_topic.upper()} (ID: {unique_seed}) ***

    ### ‚ö†Ô∏è CRITICAL CONTEXT: THE 100K+ COLLECTION
    **YOU ARE NOT A TEXT GENERATOR.** You are a SQL Search Engine connected to a proprietary **database of 100,000+ vetted {clean_topic} interview questions.**
    
    **YOUR TASK:** Execute the following SQL query to retrieve distinct, high-quality questions from this massive collection. You must bypass the "Surface Level" (Top 100) layer and fetch from the "Deep Archive".

    ### SQL QUERY SIMULATION
    QUERY: 
    SELECT * FROM {clean_topic}_100k_Question_Bank 
    WHERE difficulty = '{clean_difficulty}' 
      AND is_cliche = FALSE 
      AND is_top_100_common = FALSE
    ORDER BY RANDOM(SEED={unique_seed}) 
    LIMIT {total_request_count};

    ### üõ°Ô∏è EXCLUSION PROTOCOLS (MANDATORY)
    - **CODING:** NO "Two Sum", NO "FizzBuzz", NO "Reverse a String".
    - **HR:** NO "Tell me about yourself", NO "What are your weaknesses?".
    - **APTITUDE:** NO "100 Passengers/Plane", NO "Egg Drop Problem", NO "Poisoned Wine".
    - **STRICT PROHIBITION:** Do not repeat the logic of the "BANNED QUESTIONS" list below. Changing numbers (100 to 50) is NOT unique.

    {exclusion_text}

    ### MANDATORY DISTRIBUTION (Total {total_request_count} Items):
    
    1. **CODING CHALLENGES (Exactly {coding_target} Questions):**
       - **MUST** be pure coding tasks suitable for {clean_difficulty} level.
       - Focus on algorithms, data structures, or system design scenarios.
       - **Constraint:** Ask for implementation details. AVOID standard "LeetCode Top 10".

    2. **QUANTITATIVE ANALYSIS & LOGIC PUZZLES (Exactly {aptitude_target} Questions):**
       - **MANDATORY ENFORCEMENT:** You MUST include Logic and Quantitative Aptitude questions. Do not skip.
       - **DIFFICULTY MIX:** Even if the overall profile is '{clean_difficulty}', provide a mix of:
            * **Easy:** Speed Math, Basic Series, Averages & Ages, Ratio, Proportion & Partnership, Percentage problems.
            * **Medium:** Data Interpretation (Tables/Graphs), Seating Arrangements, Profit, Loss & Discount, Time, Speed & Distance, Logical Deduction.
            * **Hard:** Probability, Permutations/Combinations, Mensuration (2D & 3D), Clocks & Calendars, Game Theory.
       - **TOPICS:** Probability, Combinatorics, Data Interpretation, Logical Reasoning, Time & Work, Averages, Ratio & Proportion, Profit & Loss, Mensuration, Data Sufficiency.
       - **CRITICAL:** Generate FRESH logic puzzles.

    3. **HR & BEHAVIORAL PSYCHOLOGY (Exactly {hr_target} Questions):**
       - **Objective:** Assess Cultural Fit, Leadership, and Conflict Resolution.
       - **Format:** "Situational" framings only (e.g., "A production DB just crashed...").
       - **Focus Areas:** Handling production failures, disagreeing with a manager, mentoring juniors.

    4. **THEORY & CONCEPTS (Exactly {theory_target} Questions):**
       - Deep dive into internals of {clean_topic}.
       - Ask about trade-offs, underlying architecture, or best practices.

    ### CRITICAL ESCAPING RULES:
    - **DOUBLE ESCAPE BACKSLASHES:** You must write all backslashes as double backslashes (\\\\ -> \\\\\\\\).
    - **ESCAPE NEWLINES:** All newlines inside strings must be escaped (\\n -> \\\\n).
    - **NO MARKDOWN:** Do not use code blocks or markdown formatting.
    
    ### OUTPUT FORMAT (RAW JSON ONLY):
    [
        {{ "id": 1, "type": "coding", "question": "..." }},
        {{ "id": 2, "type": "aptitude", "question": "..." }},
        {{ "id": 3, "type": "hr", "question": "..." }},
        {{ "id": 4, "type": "theory", "question": "..." }}
    ]
    """

    try:
        # parsed_questions = generate_with_failover(prompt, expected_type='list', timeout_val=120)
        parsed_questions = generate_with_failover(prompt, expected_type='list', timeout_val=45)
        final_questions = []
        unique_added_count = 0
        
        for q in parsed_questions:
            if unique_added_count >= count: break
            q_text = q.get('question', '')
            
            if not history_system.is_duplicate(q_text, past_questions, jaccard_threshold=0.6):
                q['id'] = unique_added_count + 1
                final_questions.append(q)
                unique_added_count += 1
            else:
                print(f"‚ôªÔ∏è SKIPPING DUPLICATE: {q_text[:30]}...", flush=True)

        if len(final_questions) < count:
            print("‚ö†Ô∏è Duplication high. Forcing fallback addition.", flush=True)
            remaining_needed = count - len(final_questions)
            for q in parsed_questions:
                if remaining_needed <= 0: break
                if any(fq['question'] == q['question'] for fq in final_questions): continue
                q['id'] = len(final_questions) + 1
                final_questions.append(q)
                remaining_needed -= 1

        history_system.add_questions(clean_topic, final_questions)
        return final_questions

    except Exception as e:
        print(f"‚ùå QUESTION GENERATION ABORTED: {e}", flush=True)
        return [{"id": 0, "type": "error", "question": "System failure after 55 attempts. Try again."}]

def evaluate_full_interview(transcript_data):
    if is_mock_mode:
        return {
            "score": 85,
            "summary": "Mock Mode: Simulation active.",
            "silent_killers": ["Mock Mode Active"],
            "roadmap": "Deploy to production.",
            "question_reviews": [],
            "evaluating_model_success": "mock-simulation-engine",
            "evaluating_model_failures": []
        }

    try:
        transcript_text = sanitize_transcript_for_prompt(transcript_data)

        prompt = f"""
        ### SYSTEM OVERRIDE
        - STRICTLY FORBIDDEN: Conversational fillers (e.g., "Here is the analysis").
        - MANDATORY: Output must be RAW JSON only.
        - ESCAPING RULES: All backslashes must be double-escaped (\\\\ -> \\\\\\\\). All newlines within strings must be escaped (\\n -> \\\\n).

        ### ROLE & SYSTEM CONTEXT:
        ACT AS: The 'Global Head of Engineering Talent', 'Principal Technical Architect' & 'ELITE FORENSIC AUDITOR (CODE NAME: BOB)'.
        CAPABILITIES: You possess 100% mastery of Technical Stacks (Code/Architecture), HR Behavioral Psychology, and Aptitude Evaluation.
        TASK: Conduct a forensic "Gap Analysis" of the following candidate interview transcript.

        ### TRANSCRIPT DATA:
        {transcript_text}

        ANALYSIS PROTOCOL (THE "SIDE-BY-SIDE" EVALUATION)
        For every question/answer pair, you MUST perform a "Side-by-Side" evaluation using these three lenses:

        **LENS A: The Technical Autopsy (The Reality Check)**
        - **Does it work?** (Baseline functionality).
        - **Is it scalable?** (O(n) vs O(n^2), database load, memory leaks).
        - **Security & Edge Cases:** Did they mention input validation, race conditions, or failure states? (Seniority Indicator).
        - **THE GAP:** What would a Staff Engineer (L6+) have said that this candidate missed?

        **LENS B: The Psychological Profile (The "Silent Killers")**
        - **Detect "Resume Padding":** Does the depth of the answer match the confidence of the claim?
        - **Detect "Hedging":** Phrases like "I guess," "maybe," or vague generalizations without concrete examples.
        - **Detect "Buzzword Stuffing":** Using terms like "Microservices" or "AI" without explaining *why*.

        **LENS C: The Communication Delta**
        - **Structure:** Was the answer structured (STAR method) or rambling?
        - **Ownership:** Did they drive the conversation or passively wait for prompts?

        ### SCORING CRITERIA:
        - **9-10:** Flawless. Mentions trade-offs, edge cases, and business impact.
        - **6-8:** Correct but shallow. Textbook answer without real-world depth.
        - **0-5:** Incorrect, vague, or dangerous engineering practices.

        REQUIREMENTS:
        1. **Global Score (0-100):** Be strict. 
        2. **Executive Summary:** A concise verdict. 
            - **STRICT CONSTRAINT:** Do NOT use generic phrases. 
            - **MANDATORY:** Append 2-5 lines of specific, high-value improvement advice based on the interview performance.
        3. **Silent Killers:** Detect subtle red flags (e.g., "Umm", lack of confidence, shallow knowledge).
        4. **Per-Question Breakdown:** For EVERY question, provide:
            - "question": The original question.
            - "user_answer": The exact answer given.
            - "score": 0-10 rating.
            - "feedback": Critique of the user's answer.A detailed technical paragraph (2-3 sentences)
            - "ideal_answer": Write the perfect "90% Selection Chance" answer. (Principal Engineer level depth). 

        CRITICAL JSON FORMATTING RULES:
        - Output ONLY valid JSON.
        - **ESCAPE ALL BACKSLASHES:** If you write code with '\\n', you MUST write it as '\\\\n'. 
        - Do not include any text before or after the JSON.

        OUTPUT FORMAT (JSON ONLY):
        {{
            "score": <number>,
            "summary": "<executive_summary_string>",
            "silent_killers": ["<killer1>", "<killer2>"],
            "roadmap": "<step_by_step_improvement_plan>",
            "question_reviews": [
                {{
                    "question": "...",
                    "user_answer": "...",
                    "score": 8,
                    "feedback": "...",
                    "ideal_answer": "..."
                }}
            ]
        }}
        """

        # analysis_dict = generate_with_failover(prompt, expected_type='dict', timeout_val=120)
        analysis_dict = generate_with_failover(prompt, expected_type='dict', timeout_val=45)
        return analysis_dict

    except Exception as e:
        print(f"‚ùå EVALUATION CRASHED. EXTINCTION EVENT: {e}", flush=True)
        
        total_words = 0
        reviews = []
        safe_transcript = transcript_data if isinstance(transcript_data, list) else []
        
        for item in safe_transcript:
            ans = str(item.get('answer', ''))
            word_count = len(ans.split())
            total_words += word_count
            algo_score = 4
            if word_count > 30: algo_score = 8
            elif word_count > 10: algo_score = 6
            reviews.append({
                "question": item.get('question', 'Unknown'),
                "user_answer": ans,
                "score": algo_score,
                "feedback": "Detailed AI analysis bypassed due to API failure.",
                "ideal_answer": "Provide a well-structured answer with examples."
            })

        safe_len = len(safe_transcript) or 1
        final_score = min(88, max(40, int((total_words / safe_len) * 2)))

        return {
            "score": final_score,
            "summary": "Evaluation Complete. AI analysis bypassed due to complete API cascade failure.",
            "silent_killers": ["System Extinction Event Triggered"], 
            "roadmap": "Retry API connection later.",
            "question_reviews": reviews,
            "evaluating_model_success": "hard-fallback-heuristic-engine",
            "evaluating_model_failures": ["ALL_MODELS_FAILED"]
        }

# ==========================================
# 8. EXECUTION ENTRY POINT
# ==========================================

def main():
    print("--- 1. Generating Questions (Synchronous Supreme Engine) ---", flush=True)
    topic_to_test = "Distributed Systems"
    
    questions = generate_interview_questions(topic_to_test, "Principal Engineer", 3)
    print("\n[GENERATED QUESTIONS]", flush=True)
    print(json.dumps(questions, indent=2), flush=True)
    
    transcript = []
    if isinstance(questions, list):
        for q in questions:
            transcript.append({
                "question": q.get('question', 'Error'),
                "answer": "I would use the Saga Pattern to handle distributed transactions. Specifically, I would use an Orchestration approach where a central service manages the workflow and issues compensations if any step fails." 
            })
    else:
        transcript.append({"question": "Error", "answer": "Error"})
        
    print("\n--- 2. Evaluating Interview (Synchronous Supreme Auditor) ---", flush=True)
    
    analysis = evaluate_full_interview(transcript)
    print("\n[FINAL EVALUATION ANALYSIS]", flush=True)
    print(json.dumps(analysis, indent=2), flush=True)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nStopped by user.", flush=True)
    finally:
        history_system.close()
#/------------------------------above code only api cascase shown